{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.datasets import URLs, untar_data\n",
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, torch, re, PIL, os, mimetypes, csv, itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "from enum import Enum\n",
    "from functools import partial,reduce\n",
    "from torch import tensor\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data block API from config class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ItemList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def noop(x, *args, **kwargs): return x\n",
    "def range_of(x): return list(range(len(x)))\n",
    "torch.Tensor.ndim = property(lambda x: x.dim())\n",
    "\n",
    "import operator\n",
    "\n",
    "def test(a,b,cmp,cname=None):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
    "\n",
    "def test_eq(a,b):    test(a,b,operator.eq,'==')\n",
    "def test_ne(a,b):    test(a,b,operator.ne,'!=')\n",
    "def test_equal(a,b): test(a,b,torch.equal,'==')\n",
    "\n",
    "def compose(*funcs): return reduce(lambda f,g: lambda x: f(g(x)), reversed(funcs), noop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(noop(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def listify(o):\n",
    "    \"Make `o` a list.\"\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if not isinstance(o, Iterable): return [o]\n",
    "    #Rank 0 tensors in PyTorch are Iterable but don't have a length.\n",
    "    try: a = len(o)\n",
    "    except: return [o]\n",
    "    return list(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(listify(None),[])\n",
    "test_eq(listify([1,2,3]),[1,2,3])\n",
    "test_ne(listify([1,2,3]),[1,2,])\n",
    "test_eq(listify('abc'),['abc'])\n",
    "test_eq(listify(range(0,3)),[0,1,2])\n",
    "test_eq(listify(tensor(0)),[tensor(0)])\n",
    "test_eq(listify([tensor(0),tensor(1)]),[tensor(0),tensor(1)])\n",
    "test_eq(listify(tensor([0.,1.1])),[0,1.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def double_listify(o):\n",
    "    \"Make `o` a list of lists.\"\n",
    "    o = listify(o)\n",
    "    return o if len(o) == 0 or isinstance(o[0], list) else [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(double_listify(None),[])\n",
    "test_eq(double_listify([1,2,3]),[[1,2,3]])\n",
    "test_eq(double_listify([[1,2,3]]),[[1,2,3]])\n",
    "test_eq(double_listify([[1],[2],[3]]),[[1],[2],[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def order_sorted(funcs, order_key='_order'):\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    return sorted(listify(funcs), key=key)\n",
    "\n",
    "def apply_all(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    \"Apply all `funcs` to `x` in order, pass along `args` and `kwargs`.\"\n",
    "    for f in order_sorted(funcs, order_key=order_key): x = f(x, *args, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# basic behavior\n",
    "def _test_f1(x, a=2): return x**a\n",
    "def _test_f2(x, a=2): return a*x\n",
    "test_eq(apply_all(2, [_test_f1, _test_f2]),8)\n",
    "# order\n",
    "_test_f1._order = 1\n",
    "test_eq(apply_all(2, [_test_f1, _test_f2]),16)\n",
    "#args\n",
    "test_eq(apply_all(2, [_test_f1, _test_f2], 3),216)\n",
    "#kwargs\n",
    "test_eq(apply_all(2, [_test_f1, _test_f2], a=3),216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataSource():\n",
    "    def __init__(self, items, tfms=None, filters=None):\n",
    "        if filters is None: filters = [range(len(items))]\n",
    "        self.items,self.filters,self.tfms = listify(items),listify(filters),None\n",
    "        tfms = double_listify(tfms) #We want a list of lists of transforms.\n",
    "        for tfm in sum(tfms, []): getattr(tfm, 'setup', noop)(self)\n",
    "        tfms = [order_sorted(tfm) for tfm in tfms]\n",
    "        self.tfms = tfms\n",
    "        if len(self.tfms)==1: self.tfms = self.tfms*len(self)\n",
    "        \n",
    "    def transformed(self, tfms):\n",
    "        tfms = double_listify(tfms)\n",
    "        if len(tfms)==1: tfms = tfms*len(self)\n",
    "        tfms = [a + b for a,b in zip(self.tfms,tfms)]\n",
    "        return self.__class__(items, tfms, self.filters)\n",
    "        \n",
    "    def __len__(self): return len(self.filters)\n",
    "    def len(self, filt=0): return len(self.filters[filt])\n",
    "    def __getitem__(self, i): return FilteredList(self, i)\n",
    "\n",
    "    def sublist(self, filt):\n",
    "        return [self.get(j,filt) for j in range(self.len(filt))]\n",
    "\n",
    "    def get(self, idx, filt=0):\n",
    "        if hasattr(idx,'__len__') and getattr(idx,'ndim',1):\n",
    "            # rank>0 collection\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==self.len(filt) # bool mask\n",
    "                return [self.get(i,filt) for i,m in enumerate(idx) if m]\n",
    "            return [self.get(i,filt) for i in idx]  # index list\n",
    "        if self.filters: idx = self.filters[filt][idx]\n",
    "        res = self.items[idx]\n",
    "        if self.tfms: res = apply_all(res, self.tfms[filt])\n",
    "        return res\n",
    "    \n",
    "    def decode(self, o, filt=0):\n",
    "        if self.tfms: \n",
    "            return apply_all(o, [getattr(f, 'decode', noop) for f in reversed(self.tfms[filt])])\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range_of(self.filters):\n",
    "            yield (self.get(j,i) for j in range(self.len(i)))\n",
    "            \n",
    "    def __eq__(self,b):\n",
    "        if not isinstance(b,DataSource): b = DataSource(b)\n",
    "        if len(b) != len(self): return False\n",
    "        for i in range_of(self.filters):\n",
    "            if b.len(i) != self.len(i): return False\n",
    "            return all(self.get(j,i)==b.get(j,i) for j in range_of(self.filters[i]))\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__}\\n'\n",
    "        for i,o in enumerate(self):\n",
    "            l = self.len(i)\n",
    "            res += f'{i}: ({l} items) ['\n",
    "            res += ','.join(itertools.islice(map(str,o), 10))\n",
    "            if l>10: res += '...'\n",
    "            res += ']\\n'\n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def train(self): return self[0]\n",
    "    @property\n",
    "    def valid(self): return self[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FilteredList:\n",
    "    def __init__(self, il, filt): self.il,self.filt = il,filt\n",
    "    def __getitem__(self,i): return self.il.get(i,self.filt)\n",
    "    def __len__(self): return self.il.len(self.filt)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return (self.il.get(j,self.filt) for j in range_of(self))\n",
    "            \n",
    "    def __repr__(self):\n",
    "        res = f'({len(self)} items) ['\n",
    "        res += ','.join(itertools.islice(map(str,self), 10))\n",
    "        if len(self)>10: res += '...'\n",
    "        res += ']\\n'\n",
    "        return res\n",
    "    \n",
    "    def decode(self, o): return self.il.decode(o, self.filt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "il = DataSource(range(5))\n",
    "test_eq(il,[0,1,2,3,4])\n",
    "test_eq(il.sublist(0),[0,1,2,3,4])\n",
    "test_ne(il,[0,1,2,3,5])\n",
    "test_eq(il.get(2),2)\n",
    "test_eq(il.get([1,2]),[1,2])\n",
    "test_eq(il.get([True,False,False,True,False]),[0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "il = DataSource(range(5), lambda x:x*2)\n",
    "test_eq(il,[0,2,4,6,8])\n",
    "test_eq(il.sublist(0),[0,2,4,6,8])\n",
    "test_ne(il,[1,2,4,6,8])\n",
    "test_eq(il.get(2), 4)\n",
    "test_eq(il.get([1,2]), [2,4])\n",
    "test_eq(il.get([True,False,False,True,False]), [0,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "il = DataSource(range(5), [[noop],[lambda x:x*2]], [[1,2],[0,3,4]])\n",
    "test_eq(il.sublist(0),[1,2])\n",
    "test_eq(il.sublist(1),[0,6,8])\n",
    "test_eq(il.get(2,1), 8)\n",
    "test_eq(il.get([1,2], 1), [6,8])\n",
    "test_eq(il.get([False,True], 0), [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "fl = il[1]\n",
    "test_eq(list(fl),[0,6,8])\n",
    "test_eq(fl[2], 8)\n",
    "test_eq(fl[[1,2]], [6,8])\n",
    "test_eq(fl[[False,True,True]], [6,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uniqueify(x, sort=False, bidir=False):\n",
    "    \"Return the unique elements in `x`, optionally `sort`-ed.\"\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    if bidir: return res, {v:k for k,v in enumerate(res)}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(set(uniqueify([1,1,0,5,0,3])),{0,1,3,5})\n",
    "test_eq(uniqueify([1,1,0,5,0,3], sort=True),[0,1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(setify(None),set())\n",
    "test_eq(setify('abc'),{'abc'})\n",
    "test_eq(setify([1,2,2]),{1,2})\n",
    "test_eq(setify(range(0,3)),{0,1,2})\n",
    "test_eq(setify({1,2}),{1,2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def onehot(x, c, a=1.):\n",
    "    \"Return the `a`-hot encoded tensor for `x` with `c` classes.\"\n",
    "    res = torch.zeros(c)\n",
    "    if a<1: res += (1-a)/(c-1)\n",
    "    res[x] = a\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_equal(onehot(1,5), tensor([0.,1.,0.,0.,0.]))\n",
    "test_equal(onehot([1,3],5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_equal(onehot(tensor([1,3]),5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_equal(onehot([True,False,True,True,False],5), tensor([1.,0.,1.,1.,0.]))\n",
    "test_equal(onehot([],5), tensor([0.,0.,0.,0.,0.]))\n",
    "\n",
    "test_equal(onehot(1,5,0.9), tensor([0.025,0.9,0.025,0.025,0.025]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    \"Get all the files in `path` with optional `extensions`.\"\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        res = _get_files(path, f, extensions)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_files(path/'train'/'3')),346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.png')),346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.jpg')),0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png')),0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png', recurse=True)),709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train'])),709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train', 'test'])),729)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(im, ax=None, figsize=None, **kwargs):\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    #ax.imshow(im[0] if im.shape[0] == 1 else im.permute(1,2,0), **kwargs)\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))\n",
    "\n",
    "def get_image_files(path, include=None):\n",
    "    \"Get image files in `path` recursively.\"\n",
    "    return get_files(path, extensions=image_extensions, recurse=True, include=include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_image_files(path)),1428)\n",
    "test_eq(len(get_image_files(path/'train')),709)\n",
    "test_eq(len(get_image_files(path, include='train')),709)\n",
    "test_eq(len(get_image_files(path, include=['train','valid'])),1408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def random_splitter(items, valid_pct=0.2, seed=None):\n",
    "    \"Split `items` between train/val with `valid_pct` randomly.\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    rand_idx = torch.randperm(len(items))\n",
    "    cut = int(valid_pct * len(items))\n",
    "    return rand_idx[cut:],rand_idx[:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "trn,val = random_splitter([0,1,2,3,4,5], seed=42)\n",
    "test_equal(trn, tensor([3, 2, 4, 1, 5]))\n",
    "test_equal(val, tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_mask(items, name):\n",
    "    return [(o.parent.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-2]) == name for o in items]\n",
    "\n",
    "def grandparent_splitter(items, train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n",
    "    return _grandparent_mask(items, train_name),_grandparent_mask(items, valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#With string filenames\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = [path/'train'/'3'/'9932.png', path/'valid'/'7'/'7189.png', \n",
    "         path/'valid'/'7'/'7320.png', path/'train'/'7'/'9833.png',  \n",
    "         path/'train'/'3'/'7666.png', path/'valid'/'3'/'925.png',\n",
    "         path/'train'/'7'/'724.png', path/'valid'/'3'/'93055.png']\n",
    "trn,val = grandparent_splitter(items)\n",
    "test_eq(trn,[True,False,False,True,True,False,True,False])\n",
    "test_eq(val,[False,True,True,False,False,True,False,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_label(o):\n",
    "    \"Label `item` with the parent folder name.\"\n",
    "    return o.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-1]\n",
    "\n",
    "def re_labeller(pat):\n",
    "    \"Label `item` with regex `pat`.\"\n",
    "    pat = re.compile(pat)\n",
    "    def _inner(o):\n",
    "        res = pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{pat}\" in \"{o}\"'\n",
    "        return res.group(1)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pets DataSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TupleTransform():\n",
    "    def __init__(self, *tfms): \n",
    "        self.tfms = [order_sorted(listify(tfm)) for tfm in tfms]\n",
    "    def __call__(self,o): return [apply_all(o, tfm) for tfm in self.tfms]\n",
    "    def decode(self, o): \n",
    "        return [apply_all(x, [getattr(f, 'decode', noop) for f in reversed(tfm)]) for x,tfm in zip(o,self.tfms)]\n",
    "    \n",
    "    def setup(self,items):\n",
    "        for tfm in sum(self.tfms, []): getattr(tfm, 'setup', noop)(items)\n",
    "            \n",
    "class Transform():\n",
    "    def setup(self, items): return  # 1-time setup\n",
    "    def __call__(self,o): return o  # transform\n",
    "    def decode(self,o): return o    # reverse transform\n",
    "\n",
    "class Categorize(Transform):\n",
    "    def __init__(self, tfm=noop): self.tfm,self.vocab = tfm,None\n",
    "    def __call__(self,o): return self.o2i[self.tfm(o)]\n",
    "    def decode(self, o): return self.vocab[o]\n",
    "    def show(self, o, ax): ax.set_title(o)\n",
    "        \n",
    "    def setup(self, items):\n",
    "        if self.vocab is not None: return\n",
    "        vals = [self.tfm(o) for o in items.train]\n",
    "        self.vocab,self.o2i = uniqueify(vals, sort=True, bidir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = untar_data(URLs.PETS)/\"images\"\n",
    "items = get_image_files(source)\n",
    "split_idx = random_splitter(items)\n",
    "xt = PIL.Image.open\n",
    "yt = Categorize(re_labeller(pat = r'/([^/]+)_\\d+.jpg$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets = DataSource(items, TupleTransform(xt,yt), split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets.get(0,0)\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = pets.decode((x,y), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = show_image(x)\n",
    "yt.show(y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "TfmY = Enum('TfmY', 'Mask Image Point Bbox No')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageTransform():\n",
    "    \"Basic class for image transforms.\"\n",
    "    _order=0\n",
    "    _tfm_y_func={TfmY.Image: 'apply_img',   TfmY.Mask: 'apply_mask', TfmY.No: 'noop',\n",
    "                 TfmY.Point: 'apply_point', TfmY.Bbox: 'apply_bbox'}\n",
    "    _decode_y_func={TfmY.Image: 'unapply_img',   TfmY.Mask: 'unapply_mask', TfmY.No: 'noop',\n",
    "                   TfmY.Point: 'unapply_point', TfmY.Bbox: 'unapply_bbox'}\n",
    "    \n",
    "    def randomize(self): pass\n",
    "    \n",
    "    def __call__(self, o, **kwargs):\n",
    "        x,y = o\n",
    "        self.randomize() # Ensures we have the same state for x and y\n",
    "        self.x = x # Saves the x in case it's needed in the apply for y\n",
    "        return self.apply(x),self.apply_y(y, **kwargs)\n",
    "    \n",
    "    def decode(self, o, **kwargs):\n",
    "        (x,y) = o\n",
    "        return self.unapply(x),self.unapply_y(y, **kwargs)\n",
    "\n",
    "    def noop(self,x):         return x\n",
    "    def apply_img(self, y):   return self.apply(y)\n",
    "    def apply_mask(self, y):  return self.apply_img(y)\n",
    "    def apply_point(self, y): return y\n",
    "    def apply_bbox(self, y):  return self.apply_point(y)\n",
    "\n",
    "    def apply(self, x): return x\n",
    "    def apply_y(self, y, tfm_y=TfmY.No):\n",
    "        return getattr(self, self._tfm_y_func[tfm_y])(y)\n",
    "    \n",
    "    def unapply_img(self, y):   return self.unapply(y)\n",
    "    def unapply_mask(self, y):  return self.unapply_img(y)\n",
    "    def unapply_point(self, y): return y\n",
    "    def unapply_bbox(self, y):  return self.unapply_point(y)\n",
    "    \n",
    "    def unapply(self, x): return x\n",
    "    def unapply_y(self, y, tfm_y=TfmY.No):\n",
    "        return getattr(self, self._decode_y_func[tfm_y])(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import random\n",
    "class FakeTransform(ImageTransform):\n",
    "    def randomize(self): self.a = random.randint(1,10)\n",
    "    def apply(self, x): return x + self.a\n",
    "    def apply_mask(self, x): return x + 5\n",
    "    def apply_point(self, x): return x + 2\n",
    "\n",
    "tfm = FakeTransform()\n",
    "(x,y) = (5,10)\n",
    "#Basic behavior: x has changed, not y\n",
    "t1 = tfm((x,y))\n",
    "assert t1[0]!=x and t1[1]==y, t1\n",
    "#Check the same random integer was used for x and y when transforming y\n",
    "t1 = tfm((x,y), tfm_y=TfmY.Image)\n",
    "test_eq(t1[0] - 5,t1[1] - 10)\n",
    "#Check mask, point,bbox implementations\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Mask) [1],15)\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Point)[1],12)\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Bbox) [1],12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def ifnone(a,b): return b if a is None else a\n",
    "\n",
    "class DecodeImg(ImageTransform):\n",
    "    \"Convert regular image to RGB, masks to L mode.\"\n",
    "    def __init__(self, mode_x='RGB', mode_y=None): self.mode_x,self.mode_y = mode_x,mode_y\n",
    "    def apply(self, x):       return x.convert(self.mode_x)\n",
    "    def apply_image(self, y): return y.convert(ifnone(self.mode_y,self.mode_x))\n",
    "    def apply_mask(self, y):  return y.convert(ifnone(self.mode_y,'L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeFixed(ImageTransform):\n",
    "    \"Resize image to `size` using `mode_x` (and `mode_y` on targets).\"\n",
    "    _order=10\n",
    "    def __init__(self, size, mode_x=PIL.Image.BILINEAR, mode_y=None):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        size = (size[1],size[0]) #PIL takes size in the otherway round\n",
    "        self.size,self.mode_x,self.mode_y = size,mode_x,mode_y\n",
    "        \n",
    "    def apply(self, x):       return x.resize(self.size, self.mode_x)\n",
    "    def apply_image(self, y): return y.resize(self.size, ifnone(self.mode_y,self.mode_x))\n",
    "    def apply_mask(self, y):  return y.resize(self.size, ifnone(self.mode_y,PIL.Image.NEAREST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToByteTensor(ImageTransform):\n",
    "    \"Transform our items to byte tensors.\"\n",
    "    _order=20\n",
    "    def apply(self, x):\n",
    "        res = torch.ByteTensor(torch.ByteStorage.from_buffer(x.tobytes()))\n",
    "        w,h = x.size\n",
    "        return res.view(h,w,-1).permute(2,0,1)\n",
    "    \n",
    "    def unapply(self, x): return x[0] if x.shape[0] == 1 else x.permute(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToFloatTensor(ImageTransform):\n",
    "    \"Transform our items to float tensors (int in the case of mask).\"\n",
    "    _order=20\n",
    "    def __init__(self, div_x=255., div_y=None): self.div_x,self.div_y = div_x,div_y\n",
    "    def apply(self, x): return x.float().div_(self.div_x)\n",
    "    def apply_mask(self, x): \n",
    "        return x.long() if self.div_y is None else x.long().div_(self.div_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [DecodeImg(), ResizeFixed(128), ToByteTensor(), ToFloatTensor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pets_t = pets.transformed(tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = pets_t.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x,y) = pets_t.decode((x,y), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = show_image(x)\n",
    "yt.show(y, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def get_dls(il, bs=64):\n",
    "    return [DataLoader(il[i], bs, shuffle=i==0) for i in range_of(il)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = get_dls(pets_t, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch():\n",
    "    \"Basic wrapper around several `DataLoader`s.\"\n",
    "    def __init__(self, *dls, fns): self.dls,self.fns = dls,fns\n",
    "    def one_batch(self, i): return next(iter(self.dls[i]))\n",
    "    \n",
    "    @property\n",
    "    def train_dl(self): return self.dls[0]\n",
    "    @property\n",
    "    def valid_dl(self): return self.dls[1]\n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "\n",
    "    def show_batch(self, i, cols=3, figsize=None):\n",
    "        b = list(zip(*self.one_batch(i)))\n",
    "        rows = (len(b)+1) // cols\n",
    "        if figsize is None: figsize = (cols*3, rows*3)\n",
    "        fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "        for it,ax in zip(b,axs.flatten()):\n",
    "            it = self.dls[i].dataset.decode(it)\n",
    "            for o,fn in zip(it, self.fns): fn(o, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch(*dls, fns=(show_image,yt.show))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data.one_batch(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape,x.type(),y.shape,y.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = CategoryGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.MNIST)\n",
    "    def get_items(self, source): return get_image_files(source)\n",
    "    def split(self, items):      return grandparent_splitter(items, train_name='training', valid_name='testing')\n",
    "    def label(self, items):      return parent_labeller(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(tfms=[ToByteTensor(), ToFloatTensor()]).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmap is specified in the `item_get` for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds.x.item_get.cmap='gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PLANET_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiCategoryProcessor(CategoryProcessor):\n",
    "    \"A Processor for multi-labeled categories.\"\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item if o in self.otoi]\n",
    "    \n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]\n",
    "    \n",
    "    def create_vocab(self, items):\n",
    "        vocab = set()\n",
    "        for c in items: vocab = vocab.union(set(c))\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab.sort()\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "\n",
    "class MultiCategoryGetter(ItemGetter):\n",
    "    \"An `ItemGetter` suitable for multi-label classification targets.\"\n",
    "    default_proc = MultiCategoryProcessor\n",
    "    \n",
    "    def get(self, o): return onehot(o, len(self.procs[0].vocab))\n",
    "    def raw(self, o): return [i for i,x in enumerate(o) if x == 1]\n",
    "    def show(self, x, ax): ax.set_title(';'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "proc = MultiCategoryProcessor()\n",
    "#Even if 'c' is the first class, vocab is sorted for reproducibility\n",
    "test_eq(proc([['c','a'], ['a','b'], ['b'], []]),[[2,0], [0,1], [1], []])\n",
    "test_eq(proc([['a','c','b'], ['a']]),[[0,2,1],[0]])\n",
    "test_eq(proc.vocab,['a','b','c'])\n",
    "test_eq(proc.deprocess([[1,0], [2]]),[['b','a'], ['c']])\n",
    "test_eq(proc.proc1(['b','a']),[1,0])\n",
    "test_eq(proc.deproc1([2,0]),['c','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_str_column(df, col_name, prefix='', suffix='', delim=None):\n",
    "    \"Read `col_name` in `df`, optionnally adding `prefix` or `suffix`.\"\n",
    "    values = df[col_name].values.astype(str)\n",
    "    values = np.char.add(np.char.add(prefix, values), suffix)\n",
    "    if delim is not None:\n",
    "        values = np.array(list(csv.reader(values, delimiter=delim)))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df = pd.DataFrame({'a': ['cat', 'dog', 'car'], 'b': ['a b', 'c d', 'a e']})\n",
    "test_equal(get_str_column(df, 'a'), np.array(['cat', 'dog', 'car']))\n",
    "test_equal(get_str_column(df, 'a', prefix='o'), np.array(['ocat', 'odog', 'ocar']))\n",
    "test_equal(get_str_column(df, 'a', suffix='.png'), np.array(['cat.png', 'dog.png', 'car.png']))\n",
    "test_equal(get_str_column(df, 'b', delim=' '), np.array([['a','b'], ['c','d'], ['a','e']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = MultiCategoryGetter\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return get_str_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return get_str_column(self.source, 'tags', delim=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData(tfms=tfms).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SegmentMaskGetter(ImageGetter):\n",
    "    \"An `ItemGetter` for segmentation mask targets.\"\n",
    "    default_tfm = TfmY.Mask\n",
    "    def __init__(self, procs=None, cmap='tab20', alpha=0.5): \n",
    "        super().__init__(procs, cmap=cmap, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamvidData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = SegmentMaskGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.CAMVID_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        path_lbl = self.source/'labels'\n",
    "        codes = np.loadtxt(self.source/'codes.txt', dtype=str)\n",
    "        return func_labeller(items, lambda x: path_lbl/f'{x.stem}_P{x.suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CamvidData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biwii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PointsGetter(ItemGetter):\n",
    "    \"An `ItemGetter` for points.\"\n",
    "    default_tfm = TfmY.Point\n",
    "    def __init__(self, procs=None, do_scale=True, y_first=False): \n",
    "        super().__init__(procs)\n",
    "        self.do_scale,self.y_first = do_scale,y_first\n",
    "    \n",
    "    def get(self, o):\n",
    "        if not isinstance(o, torch.Tensor): o = tensor(o)\n",
    "        o = o.view(-1, 2).float()\n",
    "        if not self.y_first: o = o.flip(1)\n",
    "        if self.do_scale and hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor(list(self._x.size)).float()\n",
    "            o = o * 2/sz - 1\n",
    "        return o\n",
    "    \n",
    "    def raw(self, o):\n",
    "        o = o.flip(1)\n",
    "        if hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor([self._x.shape[1:]]).float()\n",
    "            o = (o + 1) * sz/2\n",
    "        return o\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        params = {'s': 10, 'marker': '.', 'c': 'r'}\n",
    "        ax.scatter(x[:, 1], x[:, 0], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiwiData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = PointsGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.BIWI_SAMPLE)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        fn2ctr = pickle.load(open(self.source/'centers.pkl', 'rb'))\n",
    "        return func_labeller(items, lambda o:fn2ctr[o.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BiwiData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.data import get_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BBoxProcessor(MultiCategoryProcessor):\n",
    "    def __call__(self, items): \n",
    "        if self.vocab is None:\n",
    "            vocab = set()\n",
    "            for c in items: vocab = vocab.union(set(c[1]))\n",
    "            self.vocab = ['background'] + list(vocab)\n",
    "            self.vocab.sort()\n",
    "            self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return item[0],super().proc1(item[1])\n",
    "    def deproc1(self, idx): return idx[0],super().deproc1(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "def _draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def _draw_rect(ax, b, color='white', text=None, text_size=14):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBoxGetter(PointsGetter):\n",
    "    default_proc = BBoxProcessor\n",
    "    default_tfm = TfmY.Bbox\n",
    "     \n",
    "    def get(self, o): return super().get(o[0]).view(-1,4),o[1]\n",
    "    def raw(self, o): return super().raw(o[0].view(-1,2)).view(-1,4),o[1]\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        bbox,label = x\n",
    "        for b,l in zip(bbox, label): \n",
    "            if l != 'background': _draw_rect(ax, [b[1],b[0],b[3]-b[1],b[2]-b[0]], text=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb_pad_collate(samples, pad_idx=0):\n",
    "    max_len = max([len(s[1][1]) for s in samples])\n",
    "    bboxes = torch.zeros(len(samples), max_len, 4)\n",
    "    labels = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    imgs = []\n",
    "    for i,s in enumerate(samples):\n",
    "        imgs.append(s[0][None])\n",
    "        bbs, lbls = s[1]\n",
    "        if not (bbs.nelement() == 0):\n",
    "            bboxes[i,-len(lbls):] = bbs\n",
    "            labels[i,-len(lbls):] = tensor(lbls)\n",
    "    return torch.cat(imgs,0), (bboxes,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = BBoxGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.COCO_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'train')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        images, lbl_bbox = get_annotations(self.source/'train.json')\n",
    "        img2bbox = dict(zip(images, lbl_bbox))\n",
    "        return func_labeller(items, lambda o:img2bbox[o.name])\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        kwargs['collate_fn'] = bb_pad_collate\n",
    "        return super().databunch(bs=bs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CocoData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python notebook2script.py \"200_datablock_config.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
