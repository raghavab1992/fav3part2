{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nb_200 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking different tokening approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastai v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = (TextList.from_folder(path, processor=[OpenFileProcessor(), TokenizeProcessor()])\n",
    "              .filter_by_folder(include=['train', 'test', 'unsup']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opener = OpenFileProcessor()\n",
    "opener.process(il)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tokenizer.process(il)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory peak at 3.42G (389MB without the kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev_course nb 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [read_file(f) for f in il.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time tokens = tp(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peak at 4.6G\n",
    "\n",
    "Doesn't kill process each time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(fnames):\n",
    "    for fn in fnames:\n",
    "        with open(fn, 'r') as r:\n",
    "            txt = r.read()\n",
    "            for fn in default_pre_rules:\n",
    "                txt = fn(txt)\n",
    "            yield txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(fnames, data_queue, progress_queue, lang='en', batch_size=5000):\n",
    "    nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    for w in default_spec_tok: nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "    tokens = []\n",
    "    for docs in nlp.pipe(text_gen(fnames), batch_size=batch_size):\n",
    "        toks = [t.text for t in docs]\n",
    "        for fn in default_post_rules: toks = fn(toks)\n",
    "        tokens.append(toks)\n",
    "        progress_queue.put(1)\n",
    "    data_queue.put(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(fnames, lang='en', n_workers=4, chunk_size=5000):\n",
    "    progress_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    processes = [Process(target=process_files,\n",
    "                         args=(batch, data_queue, progress_queue, lang, chunk_size))\n",
    "                 for i,batch in enumerate(np.array_split(fnames, n_workers))]\n",
    "    for p in processes: p.start()\n",
    "    tokens = []\n",
    "    for _ in progress_bar(fnames): _ = progress_queue.get()  \n",
    "    for _ in processes: tokens += data_queue.get()\n",
    "    for p in processes: p.join()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time t = tokenize(il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing tokens in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_gen(fnames):\n",
    "    for fn in fnames:\n",
    "        with open(fn, 'r') as r:\n",
    "            txt = r.read()\n",
    "            for fn in default_pre_rules:\n",
    "                txt = fn(txt)\n",
    "            yield txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(fnames, output_file, data_queue, progress_queue, lang='en', batch_size=5000):\n",
    "    nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "    for w in default_spec_tok: nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "    counts = Counter()\n",
    "    with open(output_file, 'w') as w:\n",
    "        for docs in nlp.pipe(text_gen(fnames), batch_size=batch_size):\n",
    "            tokens = [t.text for t in docs]\n",
    "            for fn in default_post_rules: tokens = fn(tokens)\n",
    "            w.write(' '.join(tokens) + ' ')\n",
    "            progress_queue.put(1)\n",
    "            counts.update(Counter(tokens))\n",
    "    data_queue.put(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(fnames, lang='en', n_workers=4, chunk_size=5000, tokens_dir='tmp'):\n",
    "    progress_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    tokens_dir = Path(tokens_dir)\n",
    "    os.makedirs(tokens_dir, exist_ok=True)\n",
    "    processes = [Process(target=process_files,\n",
    "                         args=(batch, tokens_dir/f'tokens{i}.txt', data_queue, progress_queue, lang, chunk_size))\n",
    "                 for i,batch in enumerate(np.array_split(fnames, n_workers))]\n",
    "    \n",
    "    for p in processes: p.start()\n",
    "    counter = Counter()\n",
    "    for _ in progress_bar(range(len(fnames))): _ = progress_queue.get()  \n",
    "    for _ in processes: counter.update(data_queue.get())\n",
    "    for p in processes: p.join()\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time counter = tokenize(il.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [v for (v,c) in counter.most_common()[:60000] if c >= 2]\n",
    "for o in reversed(default_spec_tok):\n",
    "    if o in itos: itos.remove(o)\n",
    "    itos.insert(0, o)\n",
    "stoi = collections.defaultdict(int,{s:i for i,s in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are all written in a rwo (there are \\n in some texts in IMDB that might make lines, but there might not always be) and we don't want to read all in one go since the goal is to spare memory, so we read by chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chunks(file, chunk_size=1024):\n",
    "    while True:\n",
    "        data = file.read(chunk_size)\n",
    "        if not data: break\n",
    "        yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token EOS tells us when we have finished a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tokens(fname, stoi, data_queue, pid):\n",
    "    all_ids,ids,last = [],[],''\n",
    "    with open(fname, 'r') as f:\n",
    "        for chunk in read_chunks(f):\n",
    "            stream = (last+chunk).split(' ')\n",
    "            for t in stream[:-1]: #Last token is incomplete (probably) so we keep it for the next chunk\n",
    "                ids.append(stoi[t])\n",
    "                if t == EOS:\n",
    "                    all_ids.append(ids)\n",
    "                    ids = []\n",
    "            last = stream[-1]\n",
    "    data_queue.put([pid, all_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize(stoi, tokens_dir='tmp', n_workers=4):\n",
    "    data_queue = Queue(maxsize=n_workers)\n",
    "    tokens_dir = Path(tokens_dir)\n",
    "    processes = [Process(target=process_tokens,\n",
    "                         args=(tokens_dir/f'tokens{i}.txt', stoi, data_queue, i))\n",
    "                 for i in range(n_workers)]\n",
    "    for p in processes: p.start()\n",
    "    ids = [data_queue.get() for _ in processes]\n",
    "    for p in processes: p.join()\n",
    "    ids.sort(key = lambda x:x[0])\n",
    "    return np.concatenate([o[1] for o in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time ids = numericalize(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([itos[i] for i in ids[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(il.items[-1], 'r') as f: print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
