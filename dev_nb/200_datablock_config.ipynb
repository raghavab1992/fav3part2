{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.datasets import URLs, untar_data\n",
    "from pathlib import Path\n",
    "import torch, re, PIL, os, mimetypes, csv, operator, pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "from typing import *\n",
    "import pandas as pd, numpy as np\n",
    "from enum import Enum\n",
    "from torch import tensor,Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data block API from config class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def test(a,b,cmp,cname=None,tst_name=''):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{tst_name},{cname}:\\n{a}\\n{b}\"\n",
    "\n",
    "def test_eq(a,b,tst_name=''): \n",
    "    if isinstance(a, np.ndarray) or (isinstance(a, Tensor) and not len(a.shape) == 0):\n",
    "        assert len(a) == len(b), f\"{tst_name}, lengths mismatch:\\n{a}\\n{b}\"\n",
    "        test(a,b,lambda x,y: (x == y).all(),'==',tst_name)\n",
    "    else: test(a,b,operator.eq,'==',tst_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(noop(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def listify(o):\n",
    "    \"Make `o` a list.\"\n",
    "    if o is None: return []\n",
    "    if isinstance(o, list): return o\n",
    "    if isinstance(o, str): return [o]\n",
    "    if isinstance(o, Iterable): return list(o)\n",
    "    return [o]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(listify(None), [])\n",
    "test_eq(listify([1,2,3]), [1,2,3])\n",
    "test_eq(listify('abc'), ['abc'])\n",
    "test_eq(listify(range(0,3)), [0,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compose(x, funcs, *args, order_key='_order', **kwargs):\n",
    "    \"Apply all `funcs` to `x` in order, pass along `args` and `kwargs`.\"\n",
    "    key = lambda o: getattr(o, order_key, 0)\n",
    "    for f in sorted(listify(funcs), key=key): x = f(x, *args, **kwargs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# basic behavior\n",
    "def _test_f1(x, a=2): return x**a\n",
    "def _test_f2(x, a=2): return a*x\n",
    "test_eq(compose(2, [_test_f1, _test_f2]), 8)\n",
    "# order\n",
    "_test_f1._order = 1\n",
    "test_eq(compose(2, [_test_f1, _test_f2]), 16)\n",
    "#args\n",
    "test_eq(compose(2, [_test_f1, _test_f2], 3), 216)\n",
    "#kwargs\n",
    "test_eq(compose(2, [_test_f1, _test_f2], a=3), 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def uniqueify(x, sort=False):\n",
    "    \"Return the unqiue elements in `x`, optionally `sort`-ed.\"\n",
    "    res = list(OrderedDict.fromkeys(x).keys())\n",
    "    if sort: res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(set(uniqueify([1,1,0,5,0,3])), {0,1,3,5})\n",
    "test_eq(uniqueify([1,1,0,5,0,3], sort=True), [0,1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def setify(o): return o if isinstance(o,set) else set(listify(o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(setify(None), set())\n",
    "test_eq(setify('abc'), {'abc'})\n",
    "test_eq(setify([1,2,2]), {1,2})\n",
    "test_eq(setify(range(0,3)), {0,1,2})\n",
    "test_eq(setify({1,2}), {1,2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def onehot(x, c):\n",
    "    \"Return the one-hot encoded tensor for `x` with `c` classes.\"\n",
    "    res = torch.zeros(c)\n",
    "    res[x] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(onehot(1,5), tensor([0.,1.,0.,0.,0.]))\n",
    "test_eq(onehot([1,3],5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_eq(onehot(tensor([1,3]),5), tensor([0.,1.,0.,1.,0.]))\n",
    "test_eq(onehot([True,False,True,True,False],5), tensor([1.,0.,1.,1.,0.]))\n",
    "test_eq(onehot([],5), tensor([0.,0.,0.,0.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_files(p, fs, extensions=None):\n",
    "    p = Path(p)\n",
    "    res = [p/f for f in fs if not f.startswith('.')\n",
    "           and ((not extensions) or f'.{f.split(\".\")[-1].lower()}' in extensions)]\n",
    "    return res\n",
    "\n",
    "def get_files(path, extensions=None, recurse=False, include=None):\n",
    "    \"Get all the files in `path` with optional `extensions`.\"\n",
    "    path = Path(path)\n",
    "    extensions = setify(extensions)\n",
    "    extensions = {e.lower() for e in extensions}\n",
    "    if recurse:\n",
    "        res = []\n",
    "        for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "            if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "            else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "            res += _get_files(p, f, extensions)\n",
    "        return res\n",
    "    else:\n",
    "        f = [o.name for o in os.scandir(path) if o.is_file()]\n",
    "        return _get_files(path, f, extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_files(path/'train'/'3')), 346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.png')), 346)\n",
    "test_eq(len(get_files(path/'train'/'3', extensions='.jpg')), 0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png')), 0)\n",
    "test_eq(len(get_files(path/'train', extensions='.png', recurse=True)), 709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train'])), 709)\n",
    "test_eq(len(get_files(path, extensions='.png', recurse=True, include=['train', 'test'])), 729)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def grab_idx(batch, i):\n",
    "    \"Return the `i`-th sample in `batch`\"\n",
    "    return [grab_idx(b,i) for b in batch] if isinstance(batch, (list,tuple)) else batch[i].detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(grab_idx(tensor([1,2]), 1), 2)\n",
    "test_eq(grab_idx([tensor([1,2]), tensor([3,4])], 1), [2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_column(df, col_name, prefix='', suffix='', delim=None):\n",
    "    \"Read `col_name` in `df`, optionnally adding `prefix` or `suffix`.\"\n",
    "    values = df[col_name].values.astype(str)\n",
    "    values = np.char.add(np.char.add(prefix, values), suffix)\n",
    "    if delim is not None:\n",
    "        values = np.array(list(csv.reader(values, delimiter=delim)))\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "df = pd.DataFrame({'a': ['cat', 'dog', 'car'], 'b': ['a b', 'c d', 'a e']})\n",
    "test_eq(read_column(df, 'a'), np.array(['cat', 'dog', 'car']))\n",
    "test_eq(read_column(df, 'a', prefix='o'), np.array(['ocat', 'odog', 'ocar']))\n",
    "test_eq(read_column(df, 'a', suffix='.png'), np.array(['cat.png', 'dog.png', 'car.png']))\n",
    "test_eq(read_column(df, 'b', delim=' '), np.array([['a','b'], ['c','d'], ['a','e']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor and ItemGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Processor():\n",
    "    \"A basic class to preprocess some data.\"\n",
    "    def __call__(self, items): return items\n",
    "    def process1(self, item):  return item\n",
    "    def deproc1(self, item):   return item\n",
    "    def deproc(self, items):   return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "TfmY = Enum('TfmY', 'No Mask Image Point Bbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ItemGetter():\n",
    "    \"A basic class for representing some data type.\"\n",
    "    #Subclass for default processor associated to this data type (example: CategoryGetter has CategoryProcessor)\n",
    "    default_proc = None\n",
    "    #Subclass for default `tfm_y` value associated to this data type (example: SegmentMaskGetter has TfmY.Mask)\n",
    "    default_tfm = TfmY.No\n",
    "    \n",
    "    def __init__(self, procs=None): \n",
    "        \"Initialize with `default_proc` if no `procs` are passed.\"\n",
    "        self.procs = [p() for p in listify(self.default_proc)] if procs is None else procs\n",
    "    def __call__(self, items): \n",
    "        \"Process data when called on it.\"\n",
    "        return compose(items, self.procs)\n",
    "    \n",
    "    def get(self, o): \n",
    "        \"How to get the actual item from `o` (example: fn -> Image, idxs -> 1hot encoded tensor).\"\n",
    "        return o  \n",
    "    def raw(self, o): \n",
    "        \"Undoes get when needed for representation (1hot encoded tensor -> idxs).\"\n",
    "        return o  \n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        \"How to show one element `x` on `ax`.\"\n",
    "        raise NotImplementedError\n",
    "    def show_xys(self, xs, ys, y_get, **kwargs): \n",
    "        \"How to organize the show of multiple `xs` and `ys`\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "class FakeProc1(Processor): \n",
    "    def __call__(self, items):  return [a ** 2 for a in items]\n",
    "class FakeProc2(Processor): \n",
    "    def __call__(self, items):  return [a + 1  for a in items]\n",
    "    def deproc1(self, item):    return item-1\n",
    "    \n",
    "class FakeItemGetter(ItemGetter):\n",
    "    default_proc = FakeProc1\n",
    "    \n",
    "    def get(self, o): return o*2\n",
    "    def raw(self, o): return o/2\n",
    "\n",
    "#Basics\n",
    "items = [0,1,2,3,4]\n",
    "get1 = FakeItemGetter()\n",
    "test_eq(len(get1.procs), 1)\n",
    "assert isinstance(get1.procs[0], FakeProc1)\n",
    "test_eq(get1(items), [0,1,4,9,16])\n",
    "test_eq(get1.get(items[1]), 2)\n",
    "test_eq(get1.raw(4), 2)\n",
    "\n",
    "#Passing a procs override the default.\n",
    "get2 = FakeItemGetter(procs=FakeProc2())\n",
    "assert isinstance(get2.procs, FakeProc2)\n",
    "test_eq(get2(items), [1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data block API core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One ItemList to contain them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ItemList():\n",
    "    def __init__(self, items, item_get=None):\n",
    "        self.item_get = ItemGetter() if item_get is None else item_get\n",
    "        self.items = self.item_get(listify(items))\n",
    "    def _get(self, i): return self.item_get.get(i)\n",
    "    def __getitem__(self, idx):\n",
    "        try: return self._get(self.items[idx])\n",
    "        except TypeError:\n",
    "            if isinstance(idx[0],bool):\n",
    "                assert len(idx)==len(self) # bool mask\n",
    "                return [self._get(o) for m,o in zip(idx,self.items) if m]\n",
    "            return [self._get(self.items[i]) for i in idx]\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __iter__(self): return iter(self.items)\n",
    "    def __setitem__(self, i, o): self.items[i] = o\n",
    "    def __delitem__(self, i): del(self.items[i])\n",
    "    def __repr__(self):\n",
    "        res = f'{self.__class__.__name__} ({len(self)} items)\\n{self.items[:10]}'\n",
    "        if len(self)>10: res = res[:-1]+ '...]'\n",
    "        return res\n",
    "    \n",
    "    def deproc(self, item):\n",
    "        \"Calls `raw` on item then `deproc1` for all processors.\"\n",
    "        item = self.item_get.raw(item)\n",
    "        for proc in reversed(listify(self.item_get.procs)):\n",
    "            item = proc.deproc1(item)\n",
    "        return item\n",
    "    \n",
    "    def obj(self, idx):\n",
    "        isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)\n",
    "        item = self[idx]\n",
    "        return self.deproc(item) if isint else [self.deproc(o) for o in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "il = ItemList([0,1,2,3,4], get1)\n",
    "#Items are processed\n",
    "test_eq(il.items, [0,1,4,9,16])\n",
    "#Test get and indexing\n",
    "test_eq(il[2], 8)\n",
    "test_eq(il[[1,2]], [2,8])\n",
    "test_eq(il[[True,False,False,True,False]], [0,18])\n",
    "#Test obj\n",
    "test_eq(il.obj(2), 4)\n",
    "test_eq(il.obj([1,2]), [1,4])\n",
    "test_eq(il.obj([True,False,False,True,False]), [0,9])\n",
    "#Test obj with a processor that implements deproc\n",
    "il = ItemList([0,1,2,3,4], get2)\n",
    "test_eq(il.items, [1,2,3,4,5])\n",
    "test_eq(il.obj(2), 2)\n",
    "test_eq(il.obj([1,2]), [1,2])\n",
    "test_eq(il.obj([True,False,False,True,False]), [0,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, the get function for our `ys` need to know the `x`: for instance, points need to be rescaled during the `get` method so they need to know the size of the image. Same for bounding boxes. The following context manager adds that information to `ItemList.item_get`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AddXContext():\n",
    "    \"Context manager that adds `x` to `il.item_get`.\"\n",
    "    def __init__(self, il, x): self.il,self.x = il,x \n",
    "    def __enter__(self, *args):\n",
    "        self.il.item_get._x = self.x\n",
    "        return self.il\n",
    "    def __exit__(self, *args): self.il.item_get._x = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "class FakeYItemGetter(ItemGetter):\n",
    "    default_proc = FakeProc2\n",
    "    \n",
    "    def get(self, o): return o + self._x\n",
    "    def raw(self, o): return o - self._x\n",
    "    \n",
    "get2 = FakeYItemGetter()\n",
    "ily = ItemList([0,1,2,3,4], get2)\n",
    "with AddXContext(ily, 5):\n",
    "    test_eq(ily[1], 7)\n",
    "    test_eq(ily.obj(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `AddXContext` when requesting the `ys` in `getitem` and `deproc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LabeledData():\n",
    "    def __init__(self, x, y, tfms=None, tfm_y=TfmY.No):  \n",
    "        self.x,self.y,self.tfms,self.tfm_y = x,y,tfms,tfm_y\n",
    "    def __repr__(self):        return f'{self.__class__.__name__}\\nx: {self.x}\\ny: {self.y}\\n'\n",
    "    def __getitem__(self,idx):\n",
    "        #TODO? Make below works if idx is a mask/array/list\n",
    "        x = self.x[idx]\n",
    "        with AddXContext(self.y, x) as yil: y = yil[idx] \n",
    "        return compose((x,y), self.tfms, tfm_y=self.tfm_y)\n",
    "    def __len__(self):         return len(self.x)\n",
    "    \n",
    "    def deproc(self, o):\n",
    "        \"Calls deproc on the x and y of `o`.\"\n",
    "        x = self.x.deproc(o[0])\n",
    "        with AddXContext(self.y, x) as yil: y = yil.deproc(o[1])\n",
    "        return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "getx = FakeItemGetter(procs=FakeProc2())\n",
    "ilx = ItemList([0,1,2,3,4], getx)\n",
    "ll = LabeledData(ilx, ily)\n",
    "\n",
    "test_eq(ll[2],((2+1)*2, (2+1) + (2+1)*2))\n",
    "test_eq(ll.deproc((10,10)),(10/2 - 1, 10-1 - (10/2-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloader and DataBunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "def get_dl(ds, bs, shuffle=False, drop_last=False, **kwargs):\n",
    "    \"Basic function to get a `DataLoader`\"\n",
    "    return DataLoader(ds, batch_size=bs, shuffle=shuffle, drop_last=drop_last, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBunch():\n",
    "    \"Basic wrapper around several `DataLoader`.\"\n",
    "    def __init__(self, train_dl, valid_dl):\n",
    "        self.train_dl,self.valid_dl = train_dl,valid_dl\n",
    "        \n",
    "    @property\n",
    "    def train_ds(self): return self.train_dl.dataset\n",
    "    @property\n",
    "    def valid_ds(self): return self.valid_dl.dataset\n",
    "    \n",
    "    def show_batch(self, batch = None, is_valid=False, items=9, **kwargs):\n",
    "        \"Show `items` element of a batch, depending on `is_valid` for the dl it draw from.\"\n",
    "        if batch is None: batch = next(iter(self.valid_dl if is_valid else self.train_dl))\n",
    "        xs,ys = [],[]\n",
    "        for i in range(items):\n",
    "            x,y = self.train_ds.deproc((grab_idx(batch[0], i), grab_idx(batch[1], i)))\n",
    "            xs.append(x); ys.append(y)\n",
    "        self.train_ds.x.item_get.show_xys(xs, ys, self.train_ds.y.item_get, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main class to represent any kind of data. User provides the `get_x_cls` and `get_y_cls` then the four functions. At init everything is constructed with optionals transforms or custom instances of `get_x`/`get_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DataBlock():\n",
    "    \"Main class to represent a dataset. Subclass the 2 properties and 4 methods below to your need.\"\n",
    "    get_x_cls = ItemGetter #Type of input\n",
    "    get_y_cls = ItemGetter #Type of targer\n",
    "    def get_source(self):         \n",
    "        \"Return the source of your data (path, dataframe...), optionally download it.\"\n",
    "        raise NotImplementedError\n",
    "    def get_items(self, source):  \n",
    "        \"Use `source` to return the list of all items.\"\n",
    "        raise NotImplementedError\n",
    "    def split(self, items):       \n",
    "        \"Explain how so split the `items`. Return two disjoint lists of indices or two boolean masks.\"\n",
    "        raise NotImplementedError\n",
    "    def label(self, items):       \n",
    "        \"Explain how to label your `items`. Return a list of labels.\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __init__(self, tfms=None, tfm_y=None, get_x=None, get_y=None):\n",
    "        self.source = self.get_source()\n",
    "        items = ItemList(self.get_items(self.source)) #Just for fancy indexing\n",
    "        split_idx = self.split(items)\n",
    "        labels = ItemList(self.label(items))          #Just for fancy indexing\n",
    "        if get_x is None: get_x = self.get_x_cls()\n",
    "        if get_y is None: get_y = self.get_y_cls()\n",
    "        x_train,x_valid = map(lambda o: ItemList(items[o],  item_get=get_x), split_idx)\n",
    "        y_train,y_valid = map(lambda o: ItemList(labels[o], item_get=get_y), split_idx)\n",
    "        if tfm_y is None: tfm_y = get_y.default_tfm\n",
    "        self.train = LabeledData(x_train, y_train, tfms=tfms, tfm_y=tfm_y)\n",
    "        self.valid = LabeledData(x_valid, y_valid, tfms=tfms, tfm_y=tfm_y)\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        \"How to convert to a `DataBunch`. Subclass if needed.\"\n",
    "        dls = [get_dl(ds, bs, shuffle=s, drop_last=s, **kwargs) for (ds, s) in zip([self.train, self.valid], [True,False])]\n",
    "        return DataBunch(*dls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First ItemGetters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ImageGetter(ItemGetter):\n",
    "    \"An `ItemGetter` for image types.\"\n",
    "    default_tfm = TfmY.Image\n",
    "    def __init__(self, procs=None, cmap=None, alpha=1.): \n",
    "        super().__init__(procs)\n",
    "        self.cmap,self.alpha = cmap,alpha\n",
    "    def get(self, fn): return PIL.Image.open(fn)\n",
    "    def show(self, x, ax):\n",
    "        ax.imshow(x[0] if x.shape[0] == 1 else x.permute(1,2,0), cmap=self.cmap, alpha=self.alpha)\n",
    "        ax.axis('off')\n",
    "    def show_xys(self, xs, ys, y_get, cols=3, figsize=None):\n",
    "        rows = (len(xs) // cols if len(xs)%cols == 0 else len(xs)//cols+1) \n",
    "        if figsize is None: figsize = (cols*3, rows*3)\n",
    "        fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "        for x,y,ax in zip(xs, ys, axs.flatten()):\n",
    "            self.show(x, ax)\n",
    "            y_get.show(y, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CategoryProcessor(Processor):\n",
    "    \"A `Processor` for categories.\"\n",
    "    def __init__(self): self.vocab=None\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None: self.create_vocab(items)\n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return self.otoi[item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return self.vocab[idx]\n",
    "    \n",
    "    def create_vocab(self, items):\n",
    "        \"Create the `vocab` from `items`.\"\n",
    "        self.vocab = uniqueify(items, sort=True)\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "proc = CategoryProcessor()\n",
    "#Even if 'dog' is the first class, vocab is sorted for reproducibility\n",
    "test_eq(proc(['dog', 'cat', 'cat', 'dog', 'cat', 'dog']),[1,0,0,1,0,1])\n",
    "test_eq(proc(['cat', 'cat', 'dog']),[0,0,1])\n",
    "test_eq(proc.vocab,['cat', 'dog'])\n",
    "test_eq(proc.deprocess([1,0,1]),['dog', 'cat', 'dog'])\n",
    "test_eq(proc.proc1('cat'),0)\n",
    "test_eq(proc.deproc1(1),'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CategoryGetter(ItemGetter):\n",
    "    \"An `ItemGetter` suitable for single-label classification targets\"\n",
    "    default_proc = CategoryProcessor\n",
    "    def show(self, x, ax): ax.set_title(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get image files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_image_files(path, include=None):\n",
    "    \"Get image files in `path` recursively.\"\n",
    "    return get_files(path, extensions=image_extensions, recurse=True, include=include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "test_eq(len(get_image_files(path)),1428)\n",
    "test_eq(len(get_image_files(path/'train')),709)\n",
    "test_eq(len(get_image_files(path, include='train')),709)\n",
    "test_eq(len(get_image_files(path, include=['train','valid'])),1408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def random_splitter(items, valid_pct=0.2, seed=None):\n",
    "    \"Split `items` between train/val with `valid_pct` randomly.\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    rand_idx = torch.randperm(len(items))\n",
    "    cut = int(valid_pct * len(items))\n",
    "    return rand_idx[cut:],rand_idx[:cut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "trn,val = random_splitter([0,1,2,3,4,5], seed=42)\n",
    "test_eq(trn, tensor([3, 2, 4, 1, 5]))\n",
    "test_eq(val, tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _grandparent_mask(items, name):\n",
    "    return [(o.parent.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-2]) == name for o in items]\n",
    "\n",
    "def grandparent_splitter(items, train_name='train', valid_name='valid'):\n",
    "    \"Split `items` from the grand parent folder names (`train_name` and `valid_name`).\"\n",
    "    return _grandparent_mask(items, train_name),_grandparent_mask(items, valid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST_TINY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#With string filenames\n",
    "path = untar_data(URLs.MNIST_TINY)\n",
    "items = [path/'train'/'3'/'9932.png', path/'valid'/'7'/'7189.png', \n",
    "         path/'valid'/'7'/'7320.png', path/'train'/'7'/'9833.png',  \n",
    "         path/'train'/'3'/'7666.png', path/'valid'/'3'/'925.png',\n",
    "         path/'train'/'7'/'724.png', path/'valid'/'3'/'93055.png']\n",
    "trn,val = grandparent_splitter(items)\n",
    "test_eq(trn,[True,False,False,True,True,False,True,False])\n",
    "test_eq(val,[False,True,True,False,False,True,False,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parent_labeller(items):\n",
    "    \"Label `items` with the parent folder name.\"\n",
    "    return [o.parent.name if isinstance(o, Path) else o.split(os.path.sep)[-1] for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(parent_labeller(items),['3','7','7','7','3','3','7','3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def func_labeller(items, func):\n",
    "    \"Label `items` according to `func`.\"\n",
    "    return [func(o) for o in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "test_eq(func_labeller(items, lambda x: int(x.parent.name)+1),[4,8,8,8,4,4,8,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def re_labeller(items, pat):\n",
    "    \"Label `items` with a regex `pat`.\"\n",
    "    pat = re.compile(pat)\n",
    "    def _inner(o):\n",
    "        res = pat.search(str(o))\n",
    "        assert res,f'Failed to find \"{pat}\" in \"{o}\"'\n",
    "        return res.group(1)\n",
    "    return func_labeller(items, _inner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "pat = re.compile(r'/([^/]+)/\\d+.png$')\n",
    "test_eq(re_labeller(items, pat),['3','7','7','7','3','3','7','3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetsData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = CategoryGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.PETS)\n",
    "    def get_items(self, source): return get_image_files(source/\"images\")\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return re_labeller(items, pat = r'/([^/]+)_\\d+.jpg$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,cls = data.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train.y.obj(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Transform(): \n",
    "    \"Basic class for data augmentation transforms.\"\n",
    "    _order=0\n",
    "    _tfm_y_func={TfmY.Image: 'apply_img',   TfmY.Mask: 'apply_mask', TfmY.No: 'noop',\n",
    "                 TfmY.Point: 'apply_point', TfmY.Bbox: 'apply_bbox'}\n",
    "    \n",
    "    def apply(self, x):       return x\n",
    "    def apply_img(self, y):   return self.apply(y)\n",
    "    def apply_mask(self, y):  return self.apply_img(y)\n",
    "    def apply_point(self, y): return y\n",
    "    def apply_bbox(self, y):  return self.apply_point(y)\n",
    "    \n",
    "    def randomize(self): pass\n",
    "    \n",
    "    def __call__(self, o, tfm_y=TfmY.No):\n",
    "        (x,y) = o\n",
    "        self.x = x #Saves the x in case it's needed in the apply for y (x.size for apply_point for instance)\n",
    "        self.randomize() #Ensures we have the same state for x and y\n",
    "        return self.apply(x),getattr(self, self._tfm_y_func[tfm_y], noop)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "import random\n",
    "class FakeTransform(Transform):\n",
    "    def randomize(self): self.a = random.randint(1,10)\n",
    "    def apply(self, x): return x + self.a\n",
    "    def apply_mask(self, x): return x + 5\n",
    "    def apply_point(self, x): return x + 2\n",
    "\n",
    "tfm = FakeTransform()\n",
    "(x,y) = (5,10)\n",
    "#Basic behavior: x has changed, not y\n",
    "t1 = tfm((x,y))\n",
    "test_eq(t1[0] != x and t1[1],y)\n",
    "#Check the same random integer was used for x and y when transforming y\n",
    "t1 = tfm((x,y), tfm_y=TfmY.Image)\n",
    "test_eq(t1[0] - 5,t1[1] - 10)\n",
    "#Check mask, point,bbox implementations\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Mask) [1],15)\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Point)[1],12)\n",
    "test_eq(tfm((x,y), tfm_y=TfmY.Bbox) [1],12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DecodeImg(Transform):\n",
    "    \"Convert regular image to RGB, masks to L mode.\"\n",
    "    def __init__(self, mode_x='RGB', mode_y=None):\n",
    "        self.mode_x,self.mode_y = mode_x,mode_y\n",
    "        \n",
    "    def apply(self, x):       return x.convert(self.mode_x)\n",
    "    def apply_image(self, y): return y.convert(self.mode_x if self.mode_y is None else self.mode_y)\n",
    "    def apply_mask(self, y):  return y.convert('L' if self.mode_y is None else self.mode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ResizeFixed(Transform):\n",
    "    \"Resize image to `size` using `mode_x` (and `mode_y` on targets).\"\n",
    "    _order=10\n",
    "    def __init__(self, size, mode_x=PIL.Image.BILINEAR, mode_y=None):\n",
    "        if isinstance(size,int): size=(size,size)\n",
    "        size = (size[1],size[0]) #PIL takes size in the otherway round\n",
    "        self.size,self.mode_x,self.mode_y = size,mode_x,mode_y\n",
    "        \n",
    "    def apply(self, x):       return x.resize(self.size, self.mode_x)\n",
    "    def apply_image(self, y): return y.resize(self.size, self.mode_x if self.mode_y is None else self.mode_y)\n",
    "    def apply_mask(self, y):  return y.resize(self.size, PIL.Image.NEAREST if self.mode_y is None else self.mode_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToByteTensor(Transform):\n",
    "    \"Transform our items to byte tensors.\"\n",
    "    _order=20\n",
    "    \n",
    "    def apply(self, x):\n",
    "        res = torch.ByteTensor(torch.ByteStorage.from_buffer(x.tobytes()))\n",
    "        w,h = x.size\n",
    "        return res.view(h,w,-1).permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ToFloatTensor(Transform):\n",
    "    \"Transform our items to float tensors (int in the case of mask).\"\n",
    "    _order=20\n",
    "    def __init__(self, div_x=255., div_y=None):\n",
    "        self.div_x,self.div_y = div_x,div_y\n",
    "    def apply(self, x):      return x.float().div_(self.div_x)\n",
    "    def apply_mask(self, x): \n",
    "        return x.long() if self.div_y is None else x.long().div_(self.div_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [DecodeImg(), ResizeFixed(128), ToByteTensor(), ToFloatTensor()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PetsData(tfms=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datab = data.databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datab.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(datab.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = CategoryGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.MNIST)\n",
    "    def get_items(self, source): return get_image_files(source)\n",
    "    def split(self, items):      return grandparent_splitter(items, train_name='training', valid_name='testing')\n",
    "    def label(self, items):      return parent_labeller(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MnistData(tfms=[ToByteTensor(), ToFloatTensor()]).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cmap is specified in the `item_get` for inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds.x.item_get.cmap='gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Planet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PLANET_SAMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiCategoryProcessor(CategoryProcessor):\n",
    "    \"A Processor for multi-labeled categories.\"\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item if o in self.otoi]\n",
    "    \n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]\n",
    "    \n",
    "    def create_vocab(self, items):\n",
    "        vocab = set()\n",
    "        for c in items: vocab = vocab.union(set(c))\n",
    "        self.vocab = list(vocab)\n",
    "        self.vocab.sort()\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "\n",
    "class MultiCategoryGetter(ItemGetter):\n",
    "    \"An `ItemGetter` suitable for multi-label classification targets.\"\n",
    "    default_proc = MultiCategoryProcessor\n",
    "    def __init__(self, procs=None, encoded=False, classes=None): \n",
    "        if procs is None and encoded: procs=[]\n",
    "        super().__init__(procs)\n",
    "        self.encoded,self.classes=encoded,classes\n",
    "    def get(self, o): return o if self.encoded else onehot(o, len(self.procs[0].vocab))\n",
    "    def raw(self, o): \n",
    "        return [self.classes[i] if self.encoded else i for i,x in enumerate(o) if x == 1]\n",
    "    def show(self, x, ax): ax.set_title(';'.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "proc = MultiCategoryProcessor()\n",
    "#Even if 'c' is the first class, vocab is sorted for reproducibility\n",
    "test_eq(proc([['c','a'], ['a','b'], ['b'], []]),[[2,0], [0,1], [1], []])\n",
    "test_eq(proc([['a','c','b'], ['a']]),[[0,2,1],[0]])\n",
    "test_eq(proc.vocab,['a','b','c'])\n",
    "test_eq(proc.deprocess([[1,0], [2]]),[['b','a'], ['c']])\n",
    "test_eq(proc.proc1(['b','a']),[1,0])\n",
    "test_eq(proc.deproc1([2,0]),['c','a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = MultiCategoryGetter\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return read_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      return read_column(self.source, 'tags', delim=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData(tfms=tfms).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = data.train_ds.y.item_get.procs[0].vocab\n",
    "otoi = {s:i for i,s in enumerate(classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanetData1(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = MultiCategoryGetter\n",
    "    \n",
    "    def get_source(self):        \n",
    "        self.path = untar_data(URLs.PLANET_SAMPLE)\n",
    "        return pd.read_csv(path/'labels.csv')\n",
    "    def get_items(self, source): return read_column(source, 'image_name', prefix=f'{self.path}/train/', suffix='.jpg')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):  \n",
    "        #This is jsut for the sake of using one-hot encoded labels, but imagine we have a dataset where it's the case.\n",
    "        tags = read_column(self.source, 'tags', delim=' ')\n",
    "        labels = []\n",
    "        for t in tags:\n",
    "            x = torch.zeros(len(classes))\n",
    "            idx = [otoi.get(l,None) for l in t]\n",
    "            idx = [i for i in idx if i is not None]\n",
    "            x[idx] = 1.\n",
    "            labels.append(x)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PlanetData1(tfms=tfms, get_y=MultiCategoryGetter(encoded=True, classes=classes)).databunch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camvid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SegmentMaskGetter(ImageGetter):\n",
    "    \"An `ItemGetter` for segmentation mask targets.\"\n",
    "    default_tfm = TfmY.Mask\n",
    "    def __init__(self, procs=None, cmap='tab20', alpha=0.5): \n",
    "        super().__init__(procs, cmap=cmap, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamvidData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = SegmentMaskGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.CAMVID_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        path_lbl = self.source/'labels'\n",
    "        codes = np.loadtxt(self.source/'codes.txt', dtype=str)\n",
    "        return func_labeller(items, lambda x: path_lbl/f'{x.stem}_P{x.suffix}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CamvidData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biwii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PointsGetter(ItemGetter):\n",
    "    \"An `ItemGetter` for points.\"\n",
    "    default_tfm = TfmY.Point\n",
    "    def __init__(self, procs=None, do_scale=True, y_first=False): \n",
    "        super().__init__(procs)\n",
    "        self.do_scale,self.y_first = do_scale,y_first\n",
    "    \n",
    "    def get(self, o):\n",
    "        \"Inner representation of point is scaled from -1 to 1 and y first.\"\n",
    "        if not isinstance(o, torch.Tensor): o = tensor(o)\n",
    "        o = o.view(-1, 2).float()\n",
    "        if not self.y_first: o = o.flip(1)\n",
    "        if self.do_scale and hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor(list(self._x.size)).float()\n",
    "            o = o * 2/sz - 1\n",
    "        return o\n",
    "    \n",
    "    def raw(self, o):\n",
    "        \"Put y second and unscale.\"\n",
    "        o = o.flip(1)\n",
    "        if hasattr(self, '_x') and self._x is not None: \n",
    "            sz = tensor([self._x.shape[1:]]).float()\n",
    "            o = (o + 1) * sz/2\n",
    "        return o\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        params = {'s': 10, 'marker': '.', 'c': 'r'}\n",
    "        ax.scatter(x[:, 1], x[:, 0], **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test \n",
    "class FakeImg():\n",
    "    def __init__(self, size): self.size = size\n",
    "fake_img = FakeImg((200,120))\n",
    "\n",
    "#Normal usage\n",
    "get = PointsGetter()\n",
    "il = ItemList([[0,0], [120,0], [0,200], [120,200], [60,100]], item_get=get)\n",
    "#Without getting any context, the points aren't scaled because they don't know the picture size.\n",
    "test_eq(il[1], tensor([[0., 120.]]))\n",
    "with AddXContext(il, fake_img):\n",
    "    test_eq(il[1], tensor([[-1., 1.]]))\n",
    "    test_eq(il[4], tensor([[0., 0.]]))\n",
    "    o = il[1]\n",
    "#Test deproc undoes the scaling and switching when providing with right context (need to be the tensor image)\n",
    "with AddXContext(il, torch.zeros(3, 120, 200)):\n",
    "    test_eq(il.deproc(o), tensor([[120.,0.]]))\n",
    "    \n",
    "#Giving scaled points\n",
    "get1 = PointsGetter(do_scale=False)\n",
    "il1 = ItemList([[-1.,-1.], [1.,-1.], [-1.,1.], [1.,1.], [0.,0.]], item_get=get1)\n",
    "with AddXContext(il, fake_img):\n",
    "    for i in range(5): test_eq(il[i], il1[i])\n",
    "with AddXContext(il1, torch.zeros(3, 120, 200)):\n",
    "    for i in range(5): test_eq(il.obj(i), il1.obj(i))\n",
    "        \n",
    "#Giving scaled points with y_first\n",
    "get2 = PointsGetter(do_scale=False, y_first=True)\n",
    "il2 = ItemList([[-1.,-1.], [-1.,1.], [1.,-1.], [1.,1.], [0.,0.]], item_get=get2)\n",
    "#Without getting any context, the points aren't scaled because they don't know the picture size.\n",
    "with AddXContext(il, fake_img):\n",
    "    for i in range(5): test_eq(il[i], il2[i])\n",
    "with AddXContext(il2, torch.zeros(3, 120, 200)):\n",
    "    for i in range(5): test_eq(il.obj(i), il2.obj(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiwiData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = PointsGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.BIWI_SAMPLE)      \n",
    "    def get_items(self, source): return get_image_files(source/'images')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        fn2ctr = pickle.load(open(self.source/'centers.pkl', 'rb'))\n",
    "        return func_labeller(items, lambda o:fn2ctr[o.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BiwiData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from fastai.vision.data import get_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BBoxProcessor(MultiCategoryProcessor):\n",
    "    \"A processor for bounding boxes.\"\n",
    "    def create_vocab(self, items):\n",
    "        super().create_vocab([c[1] for c in items])\n",
    "        self.vocab.insert(0, 'background')\n",
    "        self.otoi  = {v:k for k,v in enumerate(self.vocab)}\n",
    "\n",
    "    def proc1(self, item):  return item[0],super().proc1(item[1])\n",
    "    def deproc1(self, idx): return idx[0],super().deproc1(idx[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from matplotlib import patches, patheffects\n",
    "\n",
    "def _draw_outline(o, lw):\n",
    "    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'), patheffects.Normal()])\n",
    "\n",
    "def _draw_rect(ax, b, color='white', text=None, text_size=14):\n",
    "    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n",
    "    _draw_outline(patch, 4)\n",
    "    if text is not None:\n",
    "        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n",
    "        _draw_outline(patch,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BBoxGetter(PointsGetter):\n",
    "    \"An `ItemGetter` for bounding boxes.\"\n",
    "    default_proc = BBoxProcessor\n",
    "    default_tfm = TfmY.Bbox\n",
    "     \n",
    "    def get(self, o): return super().get(o[0]).view(-1,4),o[1]\n",
    "    def raw(self, o): return super().raw(o[0].view(-1,2)).view(-1,4),o[1]\n",
    "    \n",
    "    def show(self, x, ax):\n",
    "        bbox,label = x\n",
    "        for b,l in zip(bbox, label): \n",
    "            if l != 'background': _draw_rect(ax, [b[1],b[0],b[3]-b[1],b[2]-b[0]], text=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def bb_pad_collate(samples, pad_idx=0):\n",
    "    \"Collate function for bounding boxes targets.\"\n",
    "    max_len = max([len(s[1][1]) for s in samples])\n",
    "    bboxes = torch.zeros(len(samples), max_len, 4)\n",
    "    labels = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    imgs = []\n",
    "    for i,s in enumerate(samples):\n",
    "        imgs.append(s[0][None])\n",
    "        bbs, lbls = s[1]\n",
    "        if not (bbs.nelement() == 0):\n",
    "            bboxes[i,-len(lbls):] = bbs\n",
    "            labels[i,-len(lbls):] = tensor(lbls)\n",
    "    return torch.cat(imgs,0), (bboxes,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoData(DataBlock):\n",
    "    get_x_cls = ImageGetter\n",
    "    get_y_cls = BBoxGetter\n",
    "    \n",
    "    def get_source(self):        return untar_data(URLs.COCO_TINY)      \n",
    "    def get_items(self, source): return get_image_files(source/'train')\n",
    "    def split(self, items):      return random_splitter(items)\n",
    "    def label(self, items):      \n",
    "        images, lbl_bbox = get_annotations(self.source/'train.json')\n",
    "        img2bbox = dict(zip(images, lbl_bbox))\n",
    "        return func_labeller(items, lambda o:img2bbox[o.name])\n",
    "    \n",
    "    def databunch(self, bs=64, **kwargs):\n",
    "        kwargs['collate_fn'] = bb_pad_collate\n",
    "        return super().databunch(bs=bs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CocoData(tfms=tfms).databunch(bs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python notebook2script.py \"200_datablock_config.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
