{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/notebooks/swift/FastaiNotebooks\")\n",
      "\t\tFastaiNotebooks\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpdu64dpmu\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 1.27s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebooks' (3 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebooks\")' FastaiNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let mnist = MnistDataset(path: Path.home/\".fastai\"/\"data\"/\"mnist_tst\")\n",
    "var xTrain = mnist.xTrain\n",
    "var yTrain = mnist.yTrain\n",
    "var xValid = mnist.xValid\n",
    "var yValid = mnist.yValid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainMean = xTrain.mean()\n",
    "let trainStd  = xTrain.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = normalize(xTrain, mean: trainMean, std: trainStd)\n",
    "xValid = normalize(xValid, mean: trainMean, std: trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (Int(xTrain.shape[0]),Int(xTrain.shape[1]))\n",
    "let c = yTrain.max()+1\n",
    "print(n,m,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those can't be used to define a model cause they're not Ints though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct MyModel: Layer {\n",
    "    var layer1 = Dense<Float>(inputSize: m, outputSize: nHid, activation: relu)\n",
    "    var layer2 = Dense<Float>(inputSize: nHid, outputSize: c)\n",
    "    \n",
    "    @differentiable\n",
    "    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: layer1, layer2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = MyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pred = model.applied(to: xTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross entropy loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this part of 03 to get to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let loss = softmaxCrossEntropy(logits: pred, labels: yTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically the training loop repeats over the following steps:\n",
    "- get the output of the model on a batch of inputs\n",
    "- compare the output to the labels we have and compute a loss\n",
    "- calculate the gradients of the loss with respect to every parameter of the model\n",
    "- update said parameters with those gradients to make them a little bit better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func accuracy(_ output: Tensor<Float>, _ target: Tensor<Int32>) -> Tensor<Float>{\n",
    "    let corrects = Tensor<Float>(output.argmax(squeezingAxis: 1) .== target)\n",
    "    return corrects.mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.061683334\r\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(pred, yTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.341708, 1.3575298, 0.029310167, -0.4693321, -0.59524554, 0.61489546, -2.1155276, 2.4923306, 1.4987891, 0.0150280595] TensorShape(dimensions: [64, 10])\r\n"
     ]
    }
   ],
   "source": [
    "let bs:Int32=64                         // batch size\n",
    "let xb = xTrain[0..<bs]          // a mini-batch from x\n",
    "let preds = model.applied(to: xb) //predictions\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let yb = yTrain[0..<bs]\n",
    "let loss = softmaxCrossEntropy(logits: preds, labels: yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0625\r\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let lr:Float = 0.5   // learning rate\n",
    "let epochs = 1      // how many epochs to train for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't do the training loop by hand since gradients are blocked by TF-417"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let optimizer = SGD<MyModel, Float>(learningRate: lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let trainingContext = Context(learningPhase: .training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for i in 0..<((n-1)/Int(bs)){\n",
    "        let startIdx = Int32(i) * bs\n",
    "        let endIdx = startIdx + bs\n",
    "        let xb = xTrain[startIdx..<endIdx]\n",
    "        let yb = yTrain[startIdx..<endIdx]\n",
    "        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: xb, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: yb)\n",
    "        }\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily create a swift Dataset from our arrays. It will automatically batch things for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Batch: TensorGroup{\n",
    "    let x: Tensor<Float>\n",
    "    let y: Tensor<Int32>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let train_ds:Dataset<Batch> = Dataset(elements:Batch(x:xTrain, y:yTrain)).batched(Int64(bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for batch in train_ds{\n",
    "        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: batch.x, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: batch.y)\n",
    "        }\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `Dataset` can also do the shuffle for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in 1...epochs{\n",
    "    for batch in train_ds.shuffled(){\n",
    "        let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "            let preds = model.applied(to: batch.x, in: trainingContext)\n",
    "            return softmaxCrossEntropy(logits: preds, labels: batch.y)\n",
    "        }\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: grads)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example loss function.\n",
    "// TODO: This should be moved into the TensorFlow library/APIs.\n",
    "@differentiable(vjp: _vjpSoftmaxCrossEntropy)\n",
    "func softmaxCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n",
    "    features: Tensor<Scalar>, labels: Tensor<Scalar>\n",
    ") -> Tensor<Scalar> {\n",
    "    return Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels).loss.mean()\n",
    "}\n",
    "\n",
    "@usableFromInline\n",
    "func _vjpSoftmaxCrossEntropy<Scalar: TensorFlowFloatingPoint>(\n",
    "    features: Tensor<Scalar>, labels: Tensor<Scalar>\n",
    ") -> (Tensor<Scalar>, (Tensor<Scalar>) -> (Tensor<Scalar>, Tensor<Scalar>)) {\n",
    "    let (loss, grad) = Raw.softmaxCrossEntropyWithLogits(features: features, labels: labels)\n",
    "    let batchSize = Tensor<Scalar>(features.shapeTensor[0])\n",
    "    return (loss.mean(), { v in ((v / batchSize) * grad, Tensor<Scalar>(0)) })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Example type for use with `Dataset`.\n",
    "// TODO: The usage of this should be re-evaluated.\n",
    "public struct Example<DataScalar, LabelScalar>: TensorGroup\n",
    "    where DataScalar: TensorFlowFloatingPoint,\n",
    "          LabelScalar: TensorFlowFloatingPoint {\n",
    "    public var data: Tensor<DataScalar>\n",
    "    public var labels: Tensor<LabelScalar>\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// A training loop.\n",
    "///\n",
    "/// Trains the given model at the given keypath \n",
    "public func train<M, O: Optimizer, S>(\n",
    "    _ model: inout M,\n",
    "    at variablesKeyPath: WritableKeyPath<M, M.AllDifferentiableVariables>,\n",
    "    on dataset: Dataset<Example<S, S>>,\n",
    "    using optimizer: inout O,\n",
    "    loss: @escaping @differentiable (Tensor<S>, Tensor<S>) -> Tensor<S>\n",
    ") where O.Model == M, O.Scalar == S,\n",
    "        M.Input == Tensor<S>, M.Output == Tensor<S>\n",
    "{\n",
    "    let context = Context(learningPhase: .training)\n",
    "    for batch in dataset {\n",
    "        let (x, y) = (batch.data, batch.labels)\n",
    "        let (loss, (𝛁model, _)) = model.valueWithGradient(at: y) { (model, y) -> Tensor<S> in\n",
    "            let preds = model.applied(to: x, in: context)\n",
    "            return loss(preds, y)\n",
    "        }\n",
    "        print(loss)\n",
    "        optimizer.update(&model[keyPath: variablesKeyPath], along: 𝛁model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let train_ds:Dataset<Example> = Dataset(elements:Example(data:xTrain, labels:yTrain)).batched(Int64(bs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"03_minibtach_training.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
