{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLA compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_07_batchnorm\")\n",
      "\t\tFastaiNotebook_07_batchnorm\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpbkswux0p/swift-install\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 3.46s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebook_07_batchnorm' (11 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_07_batchnorm\")' FastaiNotebook_07_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FastaiNotebook_07_batchnorm\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let plt = Python.import(\"matplotlib.pyplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glue to get XLA compilation and AD to work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct PullbackArgs<T : TensorGroup, U : TensorGroup> : TensorGroup {\n",
    "    let input: T\n",
    "    let cotangent: U\n",
    "}\n",
    "\n",
    "class CompiledFunction<Input: Differentiable & TensorGroup, Output: Differentiable & TensorGroup> {\n",
    "    let f: @differentiable (Input) -> Output\n",
    "    init(_ f: @escaping @differentiable (Input) -> Output) {\n",
    "        self.f = f\n",
    "    }\n",
    "}\n",
    "\n",
    "func xlaCompiled<T : Differentiable & TensorGroup, U : Differentiable & TensorGroup>(\n",
    "    _ fn: @escaping @differentiable (T) -> U) -> CompiledFunction<T, U>\n",
    "    where T.CotangentVector : TensorGroup, U.CotangentVector : TensorGroup {\n",
    "    let xlaCompiledFn: (T) -> U = _graph(fn, useXLA: true)\n",
    "    let xlaCompiledPullback = _graph(\n",
    "        { (pbArgs: PullbackArgs<T, U.CotangentVector>) in\n",
    "            pullback(at: pbArgs.input, in: fn)(pbArgs.cotangent) },\n",
    "        useXLA: true\n",
    "    )\n",
    "    return CompiledFunction(differentiableFunction { x in\n",
    "        (value: xlaCompiledFn(x), pullback: { v in\n",
    "            xlaCompiledPullback(PullbackArgs(input: x, cotangent: v))})\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct XLABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningMean: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningVariance: Reference<Tensor<Scalar>>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    // TODO: check why these aren't being synthesized\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "    \n",
    "    // needed for tracing    \n",
    "    struct TrainingKernelInput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "        let input: Tensor<Scalar>\n",
    "        let scale: Tensor<Scalar>\n",
    "        let offset: Tensor<Scalar>\n",
    "        let runningMean: Tensor<Scalar>\n",
    "        let runningVariance: Tensor<Scalar>\n",
    "        let momentum: Tensor<Scalar>\n",
    "        let epsilon: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    struct TrainingKernelOutput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "        let normalized: Tensor<Scalar>\n",
    "        let newRunningMean: Tensor<Scalar>\n",
    "        let newRunningVariance: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    static func trainingKernel(_ input: TrainingKernelInput) -> TrainingKernelOutput {\n",
    "        let mean = input.input.mean(alongAxes: [0, 1, 2])\n",
    "        let variance = input.input.variance(alongAxes: [0, 1, 2])\n",
    "        let invMomentum = Tensor<Scalar>(1) - input.momentum\n",
    "        let newRunningMean = input.runningMean * input.momentum + mean * invMomentum\n",
    "        let newRunningVariance = input.runningVariance * input.momentum + variance * invMomentum\n",
    "        let normalizer = rsqrt(variance + input.epsilon) * input.scale\n",
    "        let normalized = (input.input - mean) * normalizer + input.offset\n",
    "        return TrainingKernelOutput(\n",
    "            normalized: normalized,\n",
    "            newRunningMean: newRunningMean,\n",
    "            newRunningVariance: newRunningVariance\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    @noDerivative let compiledTrainingKernel: CompiledFunction<TrainingKernelInput, TrainingKernelOutput>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [featureCount])\n",
    "        self.offset = Tensor(zeros: [featureCount])\n",
    "        self.runningMean = Reference(Tensor(0))\n",
    "        self.runningVariance = Reference(Tensor(0))\n",
    "        // Compile the training kernel to a TensorFlow graph. TensorFlow will then\n",
    "        // compile it to XLA once for each set of input shapes (hopefully!).\n",
    "        self.compiledTrainingKernel = xlaCompiled(XLABatchNorm<Scalar>.trainingKernel)\n",
    "    }\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func forwardTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let kernelInput = TrainingKernelInput(\n",
    "            input: input,\n",
    "            scale: scale,\n",
    "            offset: offset,\n",
    "            runningMean: runningMean.value,\n",
    "            runningVariance: runningVariance.value,\n",
    "            momentum: Tensor(momentum),\n",
    "            epsilon: Tensor(epsilon)\n",
    "        )\n",
    "\n",
    "        let kernelOutput = compiledTrainingKernel.f(kernelInput)\n",
    "        \n",
    "        self.runningMean.value = kernelOutput.newRunningMean\n",
    "        self.runningVariance.value = kernelOutput.newRunningVariance\n",
    "        \n",
    "        return kernelOutput.normalized\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func forwardInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let normalizer = rsqrt(self.runningVariance.value + epsilon) * scale\n",
    "        return (input - self.runningMean.value) * normalizer + offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct XLARunningBatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {\n",
    "    // Configuration hyperparameters\n",
    "    @noDerivative let momentum: Scalar\n",
    "    @noDerivative let epsilon: Scalar\n",
    "    // Running statistics\n",
    "    @noDerivative let runningSum: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningSumOfSquares: Reference<Tensor<Scalar>>\n",
    "    @noDerivative let runningCount: Reference<Scalar>\n",
    "    @noDerivative let samplesSeen: Reference<Scalar>\n",
    "    // Trainable parameters\n",
    "    var scale: Tensor<Scalar>\n",
    "    var offset: Tensor<Scalar>\n",
    "    // TODO: check why these aren't being synthesized\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    @noDerivative public var delegate: LayerDelegate<Output> = LayerDelegate()\n",
    "    \n",
    "    // needed for tracing\n",
    "    struct TrainingKernelInput: TensorGroup, Differentiable, AdditiveArithmetic { // needs AA?\n",
    "        let input: Tensor<Scalar>\n",
    "        let mom: Tensor<Scalar>\n",
    "        let runningSum: Tensor<Scalar>\n",
    "        let runningSumOfSquares: Tensor<Scalar>\n",
    "        let newRunningCount: Tensor<Scalar>\n",
    "        let scale: Tensor<Scalar>\n",
    "        let offset: Tensor<Scalar>\n",
    "        let epsilon: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    struct TrainingKernelOutput: TensorGroup, Differentiable, AdditiveArithmetic {\n",
    "        let normalized: Tensor<Scalar>\n",
    "        let newRunningSum: Tensor<Scalar>\n",
    "        let newRunningSumOfSquares: Tensor<Scalar>\n",
    "    }\n",
    "    \n",
    "    static func trainingKernel(_ input: TrainingKernelInput) -> TrainingKernelOutput {\n",
    "        let sum = input.input.sum(alongAxes: [0, 1, 2])\n",
    "        let sumOfSquares = (input.input * input.input).sum(alongAxes: [0, 1, 2])\n",
    "        let invmom = Tensor<Scalar>(1) - input.mom\n",
    "        let newRunningSum = input.mom * input.runningSum + invmom * sum\n",
    "        let newRunningSumOfSquares = input.mom * input.runningSumOfSquares + invmom * sumOfSquares\n",
    "        let mean = newRunningSum / input.newRunningCount\n",
    "        let variance = newRunningSumOfSquares / input.newRunningCount - mean * mean\n",
    "        let normalizer = rsqrt(variance + input.epsilon) * input.scale\n",
    "        let normalized = (input.input - mean) * normalizer + input.offset\n",
    "        return TrainingKernelOutput(\n",
    "            normalized: normalized,\n",
    "            newRunningSum: newRunningSum,\n",
    "            newRunningSumOfSquares: newRunningSumOfSquares\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    @noDerivative let compiledTrainingKernel: CompiledFunction<TrainingKernelInput, TrainingKernelOutput>\n",
    "    \n",
    "    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = Tensor(ones: [featureCount])\n",
    "        self.offset = Tensor(zeros: [featureCount])\n",
    "        self.runningSum = Reference(Tensor(0))\n",
    "        self.runningSumOfSquares = Reference(Tensor(0))\n",
    "        self.runningCount = Reference(Scalar(0))\n",
    "        self.samplesSeen = Reference(Scalar(0))\n",
    "        // Compile the training kernel to a TensorFlow graph. TensorFlow will then\n",
    "        // compile it to XLA once for each set of input shapes (hopefully!).\n",
    "        self.compiledTrainingKernel = xlaCompiled(XLARunningBatchNorm<Scalar>.trainingKernel)\n",
    "    }\n",
    "    \n",
    "    init(featureCount: Int, epsilon: Scalar = 1e-5) {\n",
    "        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func forwardTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let (batch, channels) = (input.shape[0], Scalar(input.shape[3]))\n",
    "        // it's fine to do scalar computation outside the JIT\n",
    "        let mom = momentum / sqrt(Scalar(batch) - 1)\n",
    "        let count = Scalar(input.scalarCount).withoutDerivative() / channels\n",
    "        let newRunningCount = mom * runningCount.value + (1 - mom) * count\n",
    "        \n",
    "        let kernelInput = TrainingKernelInput(\n",
    "            input: input,\n",
    "            mom: Tensor(mom),\n",
    "            runningSum: runningSum.value,\n",
    "            runningSumOfSquares: runningSumOfSquares.value,\n",
    "            newRunningCount: Tensor(newRunningCount),\n",
    "            scale: scale,\n",
    "            offset: offset,\n",
    "            epsilon: Tensor(epsilon)\n",
    "        )\n",
    "\n",
    "        let kernelOutput = compiledTrainingKernel.f(kernelInput)\n",
    "        \n",
    "        self.runningSum.value = kernelOutput.newRunningSum\n",
    "        self.runningSumOfSquares.value = kernelOutput.newRunningSumOfSquares\n",
    "        self.runningCount.value = newRunningCount\n",
    "        self.samplesSeen.value += Scalar(batch)\n",
    "        \n",
    "        return kernelOutput.normalized\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    func forwardInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        let mean = runningSum.value / runningCount.value\n",
    "        let variance = runningSumOfSquares.value / runningCount.value - mean * mean\n",
    "        let normalizer = rsqrt(variance + epsilon) * scale\n",
    "        return (input - mean) * normalizer + offset\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ConvXBN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: FANoBiasConv2D<Scalar>\n",
    "    var norm: XLABatchNorm<Scalar>\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1){\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)\n",
    "        self.norm = XLABatchNorm(featureCount: cOut, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return norm(conv(input))\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return forward(input)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct ConvXRBN<Scalar: TensorFlowFloatingPoint>: Layer {\n",
    "    var conv: FANoBiasConv2D<Scalar>\n",
    "    var norm: XLARunningBatchNorm<Scalar>\n",
    "    typealias Input = Tensor<Scalar>\n",
    "    typealias Output = Tensor<Scalar>\n",
    "    public init(_ cIn: Int, _ cOut: Int, ks: Int = 3, stride: Int = 1){\n",
    "        // TODO (when control flow AD works): use Conv2D without bias\n",
    "        self.conv = FANoBiasConv2D(cIn, cOut, ks: ks, stride: stride, activation: relu)\n",
    "        self.norm = XLARunningBatchNorm(featureCount: cOut, epsilon: 1e-5)\n",
    "    }\n",
    "\n",
    "    @differentiable\n",
    "    func forward(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return norm(conv(input))\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Scalar>) -> Tensor<Scalar> {\n",
    "        return forward(input)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct CnnModelXBN: Layer {\n",
    "    public var convs: [ConvXBN<Float>]\n",
    "    public var pool = FAGlobalAvgPool2D<Float>()\n",
    "    public var flatten = Flatten<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        let allFilters = [channelIn] + filters\n",
    "        convs = (0..<filters.count).map { ConvXBN(allFilters[$0], allFilters[$0+1]) }\n",
    "        linear = FADense<Float>(filters.last!, nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: TF) -> TF {\n",
    "        return input.sequenced(through: convs, pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct CnnModelXRBN: Layer {\n",
    "    public var convs: [ConvXRBN<Float>]\n",
    "    public var pool = FAGlobalAvgPool2D<Float>()\n",
    "    public var flatten = Flatten<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        let allFilters = [channelIn] + filters\n",
    "        convs = (0..<filters.count).map { ConvXRBN(allFilters[$0], allFilters[$0+1]) }\n",
    "        linear = FADense<Float>(filters.last!, nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: TF) -> TF {\n",
    "        return input.sequenced(through: convs, pool, flatten, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: false, bs: 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CnnModelXBN) -> SGD<CnnModelXBN> { return SGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModelXBN { return CnnModelXBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error: Dimension -4 must be >= 0------------ 0.00% [0/118 00:00<00:00]\n",
      "\t [[{{node Fill_376}}]]: file /swift-base/swift/stdlib/public/TensorFlow/CompilerRuntime.swift, line 278\n",
      "Current stack trace:\n",
      "0    libswiftCore.so                    0x00007fc49826de00 _swift_stdlib_reportFatalErrorInFile + 115\n",
      "1    libswiftCore.so                    0x00007fc4981b606c <unavailable> + 3035244\n",
      "2    libswiftCore.so                    0x00007fc4981b615e <unavailable> + 3035486\n",
      "3    libswiftCore.so                    0x00007fc497ffda12 <unavailable> + 1231378\n",
      "4    libswiftCore.so                    0x00007fc498182d42 <unavailable> + 2825538\n",
      "5    libswiftCore.so                    0x00007fc497ffcef9 <unavailable> + 1228537\n",
      "6    libswiftTensorFlow.so              0x00007fc4953ea022 <unavailable> + 598050\n",
      "7    libswiftTensorFlow.so              0x00007fc4953e8770 checkOk(_:file:line:) + 508\n",
      "8    libswiftTensorFlow.so              0x00007fc4953eba55 <unavailable> + 604757\n",
      "9    libswiftTensorFlow.so              0x00007fc4953fd155 <unavailable> + 676181\n",
      "10   libswiftTensorFlow.so              0x00007fc495464494 <unavailable> + 1098900\n"
     ]
    }
   ],
   "source": [
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CnnModelXRBN) -> SGD<CnnModelXRBN> { return SGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModelXRBN { return CnnModelXRBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatal error: OOM when allocating tensor with shape[512,28,28,32] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n",
      "\t [[{{node Mul_4}}]]\n",
      "Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      ": file /swift-base/swift/stdlib/public/TensorFlow/CompilerRuntime.swift, line 278\n",
      "Current stack trace:\n",
      "0    libswiftCore.so                    0x00007fd40e5afe00 _swift_stdlib_reportFatalErrorInFile + 115\n",
      "1    libswiftCore.so                    0x00007fd40e4f806c <unavailable> + 3035244\n",
      "2    libswiftCore.so                    0x00007fd40e4f815e <unavailable> + 3035486\n",
      "3    libswiftCore.so                    0x00007fd40e33fa12 <unavailable> + 1231378\n",
      "4    libswiftCore.so                    0x00007fd40e4c4d42 <unavailable> + 2825538\n",
      "5    libswiftCore.so                    0x00007fd40e33eef9 <unavailable> + 1228537\n",
      "6    libswiftTensorFlow.so              0x00007fd40b72c022 <unavailable> + 598050\n",
      "7    libswiftTensorFlow.so              0x00007fd40b72a770 checkOk(_:file:line:) + 508\n",
      "8    libswiftTensorFlow.so              0x00007fd40b72da55 <unavailable> + 604757\n",
      "9    libswiftTensorFlow.so              0x00007fd40b73f155 <unavailable> + 676181\n",
      "10   libswiftTensorFlow.so              0x00007fd40b7a6494 <unavailable> + 1098900\n"
     ]
    }
   ],
   "source": [
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileprivate extension Tensor where Scalar: Numeric {\n",
    "    mutating func resetToZero() {\n",
    "        self = Tensor(zeros: shape)\n",
    "    }\n",
    "}\n",
    "/// Stochastic gradient descent (SGD) optimizer.\n",
    "///\n",
    "/// An optimizer that implements stochastic gradient descent, with support for momentum, learning\n",
    "/// rate decay, and Nesterov momentum.\n",
    "public class FASGD<Model: Layer>: Optimizer\n",
    "    where Model.AllDifferentiableVariables == Model.CotangentVector {\n",
    "    /// The learning rate.\n",
    "    public var learningRate: Float\n",
    "    /// The momentum factor. It accelerates stochastic gradient descent in the relevant direction\n",
    "    /// and dampens oscillations.\n",
    "    public var momentum: Float\n",
    "    /// The weight decay.\n",
    "    public var decay: Float\n",
    "    /// Use Nesterov momentum if true.\n",
    "    public var nesterov: Bool\n",
    "    /// The velocity state of the model\n",
    "    public var velocity: Model.AllDifferentiableVariables\n",
    "    /// The set of steps taken.\n",
    "    public var step: Int = 0\n",
    "    \n",
    "    public init(\n",
    "        for model: Model,\n",
    "        learningRate: Float = 0.01,\n",
    "        momentum: Float = 0,\n",
    "        decay: Float = 0,\n",
    "        nesterov: Bool = false\n",
    "    ) {\n",
    "        precondition(learningRate >= 0, \"Learning rate must be non-negative\")\n",
    "        precondition(momentum >= 0, \"Momentum must be non-negative\")\n",
    "        precondition(decay >= 0, \"Weight decay must be non-negative\")\n",
    "\n",
    "        self.learningRate = learningRate\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.nesterov = nesterov\n",
    "        velocity = model.allDifferentiableVariables\n",
    "        for kp in velocity.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self) {\n",
    "            velocity[keyPath: kp].resetToZero()\n",
    "        }\n",
    "        for kp in velocity.recursivelyAllWritableKeyPaths(to: Tensor<Double>.self) {\n",
    "            velocity[keyPath: kp].resetToZero()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public func update(_ model: inout Model.AllDifferentiableVariables,\n",
    "                       along direction: Model.CotangentVector) {\n",
    "        step += 1\n",
    "        let learningRate = self.learningRate * 1 / (1 + decay * Float(step))\n",
    "        for kp in model.recursivelyAllWritableKeyPaths(to: Tensor<Float>.self) {\n",
    "            velocity[keyPath: kp] =\n",
    "                momentum * velocity[keyPath: kp] - learningRate * direction[keyPath: kp]\n",
    "            if nesterov {\n",
    "                model[keyPath: kp] +=\n",
    "                    momentum * velocity[keyPath: kp] - learningRate * direction[keyPath: kp]\n",
    "            } else {\n",
    "                model[keyPath: kp] += velocity[keyPath: kp]\n",
    "            }\n",
    "        }\n",
    "        for kp in model.recursivelyAllWritableKeyPaths(to: Tensor<Double>.self) {\n",
    "            velocity[keyPath: kp] =\n",
    "                Double(momentum) * velocity[keyPath: kp] -\n",
    "                Double(learningRate) * direction[keyPath: kp]\n",
    "            if nesterov {\n",
    "                model[keyPath: kp] +=\n",
    "                    Double(momentum) * velocity[keyPath: kp] - Double(learningRate) *\n",
    "                    direction[keyPath: kp]\n",
    "            } else {\n",
    "                model[keyPath: kp] += velocity[keyPath: kp]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CnnModelBN) -> SGD<CnnModelBN> { return SGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModelBN { return CnnModelBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "traced kernel\n",
      "made it to here\n",
      "running CnnModelXBN forward----------------------- 0.00% [0/118 00:00<00:00]\n",
      "running ConvXBN forward\n",
      "input  TensorShape(dimensions: [512, 28, 28, 1])\n",
      "conv  TensorShape(dimensions: [512, 14, 14, 8])\n",
      "running compiled kernel\n",
      "ran compiled kernel\n",
      "norm  TensorShape(dimensions: [512, 14, 14, 8])\n",
      "ran ConvXBN forward\n",
      "running ConvXBN forward\n",
      "input  TensorShape(dimensions: [512, 14, 14, 8])\n",
      "conv  TensorShape(dimensions: [512, 7, 7, 16])\n",
      "running compiled kernel\n",
      "ran compiled kernel\n",
      "norm  TensorShape(dimensions: [512, 7, 7, 16])\n",
      "ran ConvXBN forward\n",
      "running ConvXBN forward\n",
      "input  TensorShape(dimensions: [512, 7, 7, 16])\n",
      "conv  TensorShape(dimensions: [512, 4, 4, 32])\n",
      "running compiled kernel\n",
      "ran compiled kernel\n",
      "norm  TensorShape(dimensions: [512, 4, 4, 32])\n",
      "ran ConvXBN forward\n",
      "running ConvXBN forward\n",
      "input  TensorShape(dimensions: [512, 4, 4, 32])\n",
      "conv  TensorShape(dimensions: [512, 2, 2, 32])\n",
      "running compiled kernel\n",
      "ran compiled kernel\n",
      "norm  TensorShape(dimensions: [512, 2, 2, 32])\n",
      "ran ConvXBN forward\n",
      "ran CnnModelXBN forward\n",
      "Fatal error: Dimension -4 must be >= 0\n",
      "\t [[{{node Fill_269}}]]: file /swift-base/swift/stdlib/public/TensorFlow/CompilerRuntime.swift, line 278\n",
      "Current stack trace:\n",
      "0    libswiftCore.so                    0x00007fefa54ecc40 _swift_stdlib_reportFatalErrorInFile + 115\n",
      "1    libswiftCore.so                    0x00007fefa5434eac <unavailable> + 3112620\n",
      "2    libswiftCore.so                    0x00007fefa5434f9e <unavailable> + 3112862\n",
      "3    libswiftCore.so                    0x00007fefa527c852 <unavailable> + 1308754\n",
      "4    libswiftCore.so                    0x00007fefa5401b82 <unavailable> + 2902914\n",
      "5    libswiftCore.so                    0x00007fefa527bd39 <unavailable> + 1305913\n",
      "6    libswiftTensorFlow.so              0x00007fefa2663a12 <unavailable> + 633362\n",
      "7    libswiftTensorFlow.so              0x00007fefa2662160 checkOk(_:file:line:) + 508\n",
      "8    libswiftTensorFlow.so              0x00007fefa2665445 <unavailable> + 640069\n",
      "9    libswiftTensorFlow.so              0x00007fefa2676b45 <unavailable> + 711493\n",
      "10   libswiftTensorFlow.so              0x00007fefa26dde84 <unavailable> + 1134212\n"
     ]
    }
   ],
   "source": [
    "// func optFunc(_ model: CnnModelXBN) -> FASGD<CnnModelXBN> { return FASGD(for: model, learningRate: 0.4) }\n",
    "// func modelInit() -> CnnModelXBN { return CnnModelXBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "// let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "// let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "// learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "//                       learner.makeAddChannel()])\n",
    "// time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CnnModelXRBN) -> FASGD<CnnModelXRBN> { return FASGD(for: model, learningRate: 0.4) }\n",
    "func modelInit() -> CnnModelXRBN { return CnnModelXRBN(channelIn: 1, nOut: 10, filters: [8, 16, 32, 32]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegates([learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std),\n",
    "                      learner.makeAddChannel()])\n",
    "time { try! learner.fit(1) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"07a_xla_compilation.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
