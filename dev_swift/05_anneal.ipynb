{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_04_callbacks\")' FastaiNotebook_04_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_04_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<BasicModel, Float>(learningRate: 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lossOutputWithGrad(\n",
    "    model: BasicModel,\n",
    "    in context: Context,\n",
    "    inputs: Tensor<Float>,\n",
    "    labels: Tensor<Int32>\n",
    ") -> (Tensor<Float>, BasicModel.Output, BasicModel.CotangentVector) {\n",
    "    var outputs: BasicModel.Output? = nil\n",
    "    let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "        let predictions = model.applied(to: inputs, in: context)\n",
    "        outputs = predictions\n",
    "        return softmaxCrossEntropy(logits: predictions, labels: labels)\n",
    "    }\n",
    "    return (loss, outputs!, grads)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.AvgMetric(metrics: [accuracy])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define two new callbacks: the Recorder to save track of the loss and our scheduled learning rate, and a ParamScheduler that can schedule any hyperparameter as long as it's registered in the state_dict of the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension Learner {\n",
    "    func makeRecorder() -> Recorder {\n",
    "        return Recorder()\n",
    "    }\n",
    "\n",
    "    public class Recorder: Delegate {\n",
    "        public var losses: [Loss] = []\n",
    "        public var lrs: [O.Scalar] = []\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) throws {\n",
    "            if learner.inTrain {\n",
    "                losses.append(learner.currentLoss)\n",
    "                lrs.append(learner.optimizer.learningRate)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let recorder = learner.makeRecorder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.AvgMetric(metrics: [accuracy]), recorder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder.losses.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Glibc\n",
    "import Foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func formatTime(_ t: Float) -> String {\n",
    "    let t = Int(t)\n",
    "    let (h,m,s) = (t/3600, (t/60)%60, t%60)\n",
    "    return h != 0 ? String(format: \"%02d:%02d:%02d\", h, m, s) : String(format: \"%02d:%02d\", m, s)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatTime(78.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public struct ProgressBar{\n",
    "    let total: Int\n",
    "    let length: Int = 50\n",
    "    let showEvery: Float = 0.02\n",
    "    let fillChar: Character = \"X\"\n",
    "    public var comment: String = \"\"\n",
    "    private var lastVal: Int = 0\n",
    "    private var waitFor: Int = 0\n",
    "    private var startTime: UInt64 = 0\n",
    "    private var lastShow: UInt64 = 0\n",
    "    private var estimatedTotal: Float = 0.0\n",
    "    private var bar: String = \"\"\n",
    "    \n",
    "    public init(_ c: Int) { total = c }\n",
    "    \n",
    "    public mutating func update(_ val: Int){\n",
    "        if val == 0 {\n",
    "            startTime = DispatchTime.now().uptimeNanoseconds\n",
    "            lastShow = startTime\n",
    "            waitFor = 1\n",
    "            update_bar(0)\n",
    "        } else if val >= lastVal + waitFor || val == total {\n",
    "            lastShow = DispatchTime.now().uptimeNanoseconds\n",
    "            let averageTime = Float(lastShow - startTime) / (1e9 * Float(val))\n",
    "            waitFor = max(Int(averageTime / (showEvery + 1e-8)), 1)\n",
    "            estimatedTotal = Float(total) * averageTime\n",
    "            update_bar(val)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public mutating func update_bar(_ val: Int){\n",
    "        lastVal = val\n",
    "        bar = String(repeating: fillChar, count: (val * length) / total)\n",
    "        bar += String(repeating: \"-\", count: length - (val * length) / total)\n",
    "        let pct = String(format: \"%.2f\", 100.0 * Float(val)/Float(total))\n",
    "        let elapsedTime = Float(lastShow - startTime) / 1e9\n",
    "        bar += \" \\(pct)% [\\(val)/\\(total) \\(formatTime(elapsedTime))<\\(formatTime(estimatedTotal))\"\n",
    "        bar += comment.isEmpty ? \"]\" : \" \\(comment)]\"\n",
    "        print(bar, terminator:\"\\r\")\n",
    "        fflush(stdout)\n",
    "    }\n",
    "    \n",
    "    public func remove(){\n",
    "        print(String(repeating: \" \", count: bar.count), terminator:\"\\r\")\n",
    "        fflush(stdout)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var tst = ProgressBar(100)\n",
    "for i in 0...100{\n",
    "    tst.update(i)\n",
    "    usleep(50000)\n",
    "}\n",
    "tst.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension Learner {\n",
    "    public class ShowProgress: Delegate {\n",
    "        var pbar: ProgressBar? = nil\n",
    "        var iter: Int = 0\n",
    "        \n",
    "        public override func epochWillStart(learner: Learner) throws{\n",
    "            pbar = ProgressBar(learner.data.train.count(where: {_ in true}))\n",
    "            iter = 0\n",
    "            pbar!.update(iter)\n",
    "        }\n",
    "        \n",
    "        public override func validationWillStart(learner: Learner) throws{\n",
    "            if pbar != nil { pbar!.remove() }\n",
    "            pbar = ProgressBar(learner.data.valid.count(where: {_ in true}))\n",
    "            iter = 0\n",
    "            pbar!.update(iter)\n",
    "        }\n",
    "        \n",
    "        public override func epochDidFinish(learner: Learner) throws{\n",
    "            if pbar != nil { pbar!.remove() }\n",
    "        }\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) throws{\n",
    "            iter += 1\n",
    "            pbar!.update(iter)\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let recorder = learner.makeRecorder()\n",
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.ShowProgress(), \n",
    "                     Learner.AvgMetric(metrics: [accuracy]), recorder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Callbacks\n",
    "\n",
    "The code below adds callbacks and defines a new training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// Simple SGD optimizer with a modifiable learning rate.\n",
    "/// Remove me when we have a new linux build that includes the TF-425 fix.\n",
    "protocol SettableOptimizer: Optimizer {\n",
    "    var learningRate: Scalar { get set }\n",
    "}\n",
    "\n",
    "public class SettableSGD<Model: Layer>: SettableOptimizer\n",
    "    where Model.AllDifferentiableVariables == Model.CotangentVector {\n",
    "    /// The learning rate.\n",
    "    public var learningRate: Float {\n",
    "        willSet(newLearningRate) {\n",
    "            precondition(newLearningRate >= 0, \"Learning rate must be non-negative\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "    public init(learningRate: Float = 0.01) {\n",
    "        precondition(learningRate >= 0, \"Learning rate must be non-negative\")\n",
    "        self.learningRate = learningRate\n",
    "    }\n",
    "\n",
    "    public func update(_ model: inout Model.AllDifferentiableVariables,\n",
    "                       along direction: Model.CotangentVector) {\n",
    "        for kp in model.recursivelyAllWritableKeyPaths(to: Tensor<Scalar>.self) {\n",
    "            model[keyPath: kp] += learningRate * direction[keyPath: kp]\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SettableSGD<BasicModel>(learningRate: 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// A non-generalized learning rate scheduler\n",
    "extension Learner where O: SettableOptimizer, O.Scalar == Float {\n",
    "\n",
    "    public class ParamScheduler: Delegate {\n",
    "        typealias ScheduleFunc = (Float) -> Float\n",
    "\n",
    "        // A learning rate schedule from step to float.\n",
    "        public var scheduler: (Float) -> Float  // TODO: switch to ScheduleFunc\n",
    "        private var step = 0\n",
    "        private var totalSteps = 0\n",
    "        \n",
    "        init(scheduler: @escaping (Float) -> Float) {\n",
    "            self.scheduler = scheduler\n",
    "        }\n",
    "        \n",
    "        override public func trainingWillStart(learner: Learner) {\n",
    "            step = 0\n",
    "            totalSteps = learner.data.train.count(where: {_ in true})\n",
    "        }\n",
    "        \n",
    "        override public func batchDidFinish(learner: Learner) {\n",
    "            learner.optimizer.learningRate = scheduler(Float(step)/Float(totalSteps))\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linearSchedule(start: Float, end: Float, pct: Float) -> Float {\n",
    "    return start + pct * (end - start)\n",
    "}\n",
    "\n",
    "func makeAnnealer(start: Float, end: Float, schedule: @escaping (Float, Float, Float) -> Float) -> (Float) -> Float { \n",
    "    return { pct in return schedule(start, end, pct) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let annealer = makeAnnealer(start: 1, end: 2, schedule: linearSchedule)\n",
    "annealer(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// COMPILER CRASH IN HERE!\n",
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)\n",
    "let recorder = learner.makeRecorder()\n",
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.ShowProgress(), \n",
    "                     Learner.AvgMetric(metrics: [accuracy]), recorder,\n",
    "                     Learner.ParamScheduler(annealer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// TODO: implement the rest of the notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
