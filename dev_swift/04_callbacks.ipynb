{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks version 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_03_minibatch_training\")' FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (Int(xTrain.shape[0]),Int(xTrain.shape[1]))\n",
    "let c = yTrain.max()+1\n",
    "print(n,m,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those can't be used to define a model cause they're not Ints though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct BasicModel: Layer {\n",
    "    public var layer1: Dense<Float>\n",
    "    public var layer2: Dense<Float>\n",
    "    \n",
    "    public init(nIn: Int, nHid: Int, nOut: Int){\n",
    "        layer1 = Dense(inputSize: nIn, outputSize: nHid, activation: relu)\n",
    "        layer2 = Dense(inputSize: nHid, outputSize: nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: layer1, layer2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = BasicModel(nIn: m, nHid: nHid, nOut: c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct DataBunch<Element> where Element: TensorGroup{\n",
    "    public var train: Dataset<Element>\n",
    "    public var valid: Dataset<Element>\n",
    "    \n",
    "    public init(train: Dataset<Element>, valid: Dataset<Element>) {\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func mnistDataBunch(path: Path = mnistPath, flat: Bool = false, bs: Int = 64\n",
    "                          ) -> DataBunch<DataBatch<Tensor<Float>, Tensor<Int32>>>{\n",
    "    let (xTrain,yTrain,xValid,yValid) = loadMNIST(path: path, flat: flat)\n",
    "    return DataBunch(train: Dataset(elements:DataBatch(xb:xTrain, yb:yTrain)).batched(Int64(bs)), \n",
    "                     valid: Dataset(elements:DataBatch(xb:xValid, yb:yValid)).batched(Int64(bs)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner (Richard's version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public enum LearnerAction: Error {\n",
    "    case skipEpoch\n",
    "    case skipBatch\n",
    "    case stop\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "/// A model learner, responsible for initializing and training a model on a given dataset.\n",
    "// NOTE: When TF-421 is fixed, make `Label` not constrained to `Differentiable`.\n",
    "public final class Learner<Label: TensorGroup,\n",
    "                           O: TensorFlow.Optimizer & AnyObject>\n",
    "    where O.Scalar: Differentiable,\n",
    "          O.Model.Input: TensorGroup\n",
    "{\n",
    "    // Common type aliases.\n",
    "    public typealias Input = Model.Input\n",
    "    public typealias Data = DataBunch<DataBatch<Input, Label>>\n",
    "    public typealias Loss = Tensor<Float>\n",
    "    public typealias Optimizer = O\n",
    "    public typealias Model = Optimizer.Model\n",
    "    public typealias Variables = Model.AllDifferentiableVariables\n",
    "    // NOTE: When TF-421 is fixed, replace with:\n",
    "    //   public typealias LossFunction = @differentiable (Model.Output, @nondiff Label) -> Loss\n",
    "    public typealias LossOutputWithGradient = (Model, Context, Input, Label\n",
    "                                 ) -> (Loss, Model.Output?, Model.CotangentVector)\n",
    "    public typealias EventHandler = (Learner) throws -> Void\n",
    "    \n",
    "    /// The dataset on which the model will be trained.\n",
    "    public var data: Data\n",
    "    /// The optimizer used for updating model parameters along gradient vectors.\n",
    "    public var optimizer: Optimizer\n",
    "    /// The function that computes a loss value when given a prediction and a label.\n",
    "    public var lossOutputWithGradient: LossOutputWithGradient\n",
    "    /// The model being trained.\n",
    "    public var model: Model\n",
    "    \n",
    "    //Is there a better way tonitiliaze those to not make them Optionals?\n",
    "    public var currentInput: Input? = nil\n",
    "    public var currentTarget: Label? = nil\n",
    "    public var currentOutput: Model.Output? = nil\n",
    "    \n",
    "    /// The number of total epochs.\n",
    "    public private(set) var epochCount: Int = .zero\n",
    "    /// The current epoch.\n",
    "    public private(set) var currentEpoch: Int = .zero\n",
    "    /// The current gradient.\n",
    "    public private(set) var currentGradient: Model.CotangentVector = .zero\n",
    "    /// The current loss.\n",
    "    public private(set) var currentLoss: Loss = .zero\n",
    "    /// In training mode or not\n",
    "    public private(set) var inTrain: Bool = false\n",
    "    /// The current epoch + iteration, float between 0.0 and epochCount\n",
    "    public private(set) var pctEpochs: Float = 0.0\n",
    "    /// The current iteration\n",
    "    public private(set) var currentIter: Int = 0\n",
    "    /// The number of iterations in the current dataset\n",
    "    public private(set) var iterCount: Int = 0\n",
    "    \n",
    "    open class Delegate {\n",
    "        public init () {}\n",
    "        \n",
    "        open func trainingWillStart(learner: Learner) throws {}\n",
    "        /// The completion of model training.\n",
    "        open func trainingDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of an epoch.\n",
    "        open func epochWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of an epoch.\n",
    "        open func epochDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of model validation.\n",
    "        open func validationWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of training on a batch.\n",
    "        open func batchWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of training on a batch.\n",
    "        open func batchDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called when a new gradient has been computed.\n",
    "        open func learnerDidProduceNewGradient(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of an optimizer update.\n",
    "        open func optimizerDidUpdate(learner: Learner) throws {}\n",
    "        \n",
    "        /// TODO: learnerDidProduceNewOutput and learnerDidProduceNewLoss need to\n",
    "        /// be differentiable once we can have the loss function inside the Learner\n",
    "    }\n",
    "    public var delegates: [Delegate] = []\n",
    "    \n",
    "    /// The context used for layer applications.\n",
    "    public private(set) var context = Context(learningPhase: .training)\n",
    "\n",
    "    /// Creates a learner.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - dataset: The dataset which will be trained on.\n",
    "    ///   - lossFunction: The loss function.\n",
    "    ///   - optimizer: The optimizer used for updating model parameters along\n",
    "    ///     gradient vectors.\n",
    "    ///   - modelInitializer: The closure that produces an model to be trained.\n",
    "    ///\n",
    "    public init(data: Data,\n",
    "                lossOutputWithGradient: @escaping LossOutputWithGradient,\n",
    "                optimizer: Optimizer,\n",
    "                initializingWith modelInitializer: () -> Model) {\n",
    "        self.data = data\n",
    "        self.optimizer = optimizer\n",
    "        self.lossOutputWithGradient = lossOutputWithGradient\n",
    "        self.model = modelInitializer()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's write the parts of the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    /// Trains the model on the given batch.\n",
    "    ///\n",
    "    /// - Parameter batch: The batch of input data and labels to be trained on.\n",
    "    ///\n",
    "    private func train(onBatch batch: DataBatch<Input, Label>) throws {\n",
    "        (currentLoss, currentOutput, currentGradient) = lossOutputWithGradient(model, context, batch.xb, batch.yb)\n",
    "        try delegates.forEach { try $0.learnerDidProduceNewGradient(learner: self) }\n",
    "        optimizer.update(&model.allDifferentiableVariables, along: self.currentGradient)\n",
    "    }\n",
    "    \n",
    "    /// Performs a training epoch on a Dataset.\n",
    "    private func train(onDataset ds: Dataset<DataBatch<Input, Label>>) throws {\n",
    "        iterCount = ds.count(where: {_ in true})\n",
    "        for batch in ds {\n",
    "            (currentInput, currentTarget) = (batch.xb, batch.yb)\n",
    "            try delegates.forEach { try $0.batchWillStart(learner: self) }\n",
    "            do { try train(onBatch: batch) }\n",
    "            catch LearnerAction.skipBatch { break }\n",
    "            try delegates.forEach { try $0.batchDidFinish(learner: self) }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the whole fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner{\n",
    "    /// Starts fitting.\n",
    "    /// - Parameter epochCount: The number of epochs that will be run.\n",
    "    public func fit(_ epochCount: Int) throws {\n",
    "        self.epochCount = epochCount\n",
    "        do {\n",
    "            try delegates.forEach { try $0.trainingWillStart(learner: self) }\n",
    "            for i in 0..<epochCount {\n",
    "                self.currentEpoch = i\n",
    "                try delegates.forEach { try $0.epochWillStart(learner: self) }\n",
    "                do { try train(onDataset: data.train) }\n",
    "                try delegates.forEach { try $0.validationWillStart(learner: self) }\n",
    "                do { try train(onDataset: data.valid) }\n",
    "                catch LearnerAction.skipEpoch { break }\n",
    "                try delegates.forEach { try $0.epochDidFinish(learner: self) }\n",
    "            }\n",
    "            try delegates.forEach { try $0.trainingDidFinish(learner: self) }\n",
    "        } catch LearnerAction.stop { return }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<BasicModel, Float>(learningRate: 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lossOutputWithGrad(\n",
    "    model: BasicModel,\n",
    "    in context: Context,\n",
    "    inputs: Tensor<Float>,\n",
    "    labels: Tensor<Int32>\n",
    ") -> (Tensor<Float>, BasicModel.Output, BasicModel.CotangentVector) {\n",
    "    var outputs: BasicModel.Output? = nil\n",
    "    let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "        let predictions = model.applied(to: inputs, in: context)\n",
    "        outputs = predictions\n",
    "        return softmaxCrossEntropy(logits: predictions, labels: labels)\n",
    "    }\n",
    "    return (loss, outputs!, grads)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add Callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback classes are defined as extensions of the Learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    public class TrainEvalDelegate: Delegate {\n",
    "        public override func trainingWillStart(learner: Learner) throws {\n",
    "            learner.pctEpochs = 0.0\n",
    "            learner.currentIter = 0\n",
    "        }\n",
    "\n",
    "        public override func epochWillStart(learner: Learner) throws {\n",
    "            //print(\"Beginning epoch \\(learner.currentEpoch)\")\n",
    "            learner.pctEpochs = Float(learner.currentEpoch)\n",
    "            learner.context = Context(learningPhase: .training)\n",
    "            learner.inTrain = true\n",
    "        }\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) throws{\n",
    "            if learner.inTrain{\n",
    "                learner.pctEpochs   += 1.0 / Float(learner.iterCount)\n",
    "                learner.currentIter += 1\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        public override func validationWillStart(learner: Learner) throws {\n",
    "            learner.context = Context(learningPhase: .inference)\n",
    "            learner.inTrain = false\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [Learner.TrainEvalDelegate()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AverageMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    public class AvgMetric: Delegate {\n",
    "        public let metrics: [(Tensor<Float>, Tensor<Int32>) -> Tensor<Float>]\n",
    "        var total: Int = 0\n",
    "        var partials: [Tensor<Float>] = []\n",
    "        \n",
    "        public init(metrics: [(Tensor<Float>, Tensor<Int32>) -> Tensor<Float>]){ self.metrics = metrics}\n",
    "        \n",
    "        public override func epochWillStart(learner: Learner) throws {\n",
    "            total = 0\n",
    "            partials = Array(repeating: Tensor(0), count: metrics.count + 1)\n",
    "        }\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) throws{\n",
    "            if !learner.inTrain{\n",
    "                if let target = learner.currentTarget as? Tensor<Int32>{\n",
    "                    let bs = target.shape[0]\n",
    "                    total += Int(bs)\n",
    "                    partials[0] += Float(bs) * learner.currentLoss\n",
    "                    for i in 1...metrics.count{\n",
    "                        partials[i] += Float(bs) * metrics[i-1]((learner.currentOutput as! Tensor<Float>), target)\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        public override func epochDidFinish(learner: Learner) throws {\n",
    "            for i in 0...metrics.count {partials[i] = partials[i] / Float(total)}\n",
    "            print(\"Epoch \\(learner.currentEpoch): \\(partials)\")\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossOutputWithGradient: lossOutputWithGrad, optimizer: opt, initializingWith: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [Learner.TrainEvalDelegate(), Learner.AvgMetric(metrics: [accuracy])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"04_callbacks.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
