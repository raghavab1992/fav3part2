/*
THIS FILE WAS AUTOGENERATED! DO NOT EDIT!
file to edit: 07_batchnorm.ipynb

*/
        
import Path
import TensorFlow
import Python

class Reference<T> {
    var value: T
    init(_ value: T) {
        self.value = value
    }
}

protocol LearningPhaseDependent: Layer {
    @differentiable func applyingTraining(to input: Input) -> Output
    @differentiable func applyingInference(to input: Input) -> Output
}

extension LearningPhaseDependent {
    func applied(to input: Input, in context: Context) -> Output {
        switch context.learningPhase {
        case .training: return applyingTraining(to: input)
        case .inference: return applyingInference(to: input)
        }
    }

    @differentiating(applied)
    func gradApplied(to input: Input, in context: Context) ->
        (value: Output, pullback: (Output.CotangentVector) ->
            (Self.CotangentVector, Input.CotangentVector)) {
        switch context.learningPhase {
        case .training:
            return valueWithPullback(at: input) {
                $0.applyingTraining(to: $1)
            }
        case .inference:
            return valueWithPullback(at: input) {
                $0.applyingInference(to: $1)
            }
        }
    }
}

protocol Norm: Layer {
    associatedtype Scalar
    typealias Input = Tensor<Scalar>
    typealias Output = Tensor<Scalar>
    init(featureCount: Int, epsilon: Scalar)
}

struct FABatchNorm<Scalar: TensorFlowFloatingPoint>: LearningPhaseDependent, Norm {
    // Configuration hyperparameters
    @noDerivative let momentum: Scalar
    @noDerivative let epsilon: Scalar
    // Running statistics
    @noDerivative let runningMean: Reference<Tensor<Scalar>>
    @noDerivative let runningVariance: Reference<Tensor<Scalar>>
    // Trainable parameters
    var scale: Tensor<Scalar>
    var offset: Tensor<Scalar>
    // TODO: check why these aren't being synthesized
    typealias Input = Tensor<Scalar>
    typealias Output = Tensor<Scalar>
    
    init(featureCount: Int, momentum: Scalar, epsilon: Scalar = 1e-5) {
        self.momentum = momentum
        self.epsilon = epsilon
        self.scale = Tensor(ones: [Int32(featureCount)])
        self.offset = Tensor(zeros: [Int32(featureCount)])
        self.runningMean = Reference(Tensor(0))
        self.runningVariance = Reference(Tensor(1))
    }
    
    init(featureCount: Int, epsilon: Scalar = 1e-5) {
        self.init(featureCount: featureCount, momentum: 0.9, epsilon: epsilon)
    }

    @differentiable
    func applyingTraining(to input: Tensor<Scalar>) -> Tensor<Scalar> {
        let mean = input.mean(alongAxes: [0, 1, 2])
        let variance = input.variance(alongAxes: [0, 1, 2])
        runningMean.value += (mean - runningMean.value) * (1 - momentum)
        runningVariance.value += (variance - runningVariance.value) * (1 - momentum)
        let normalizer = rsqrt(variance + epsilon) * scale
        return (input - mean) * normalizer + offset
    }
    
    @differentiable
    func applyingInference(to input: Tensor<Scalar>) -> Tensor<Scalar> {
        let mean = runningMean.value
        let variance = runningVariance.value
        let normalizer = rsqrt(variance + epsilon) * scale
        return (input - mean) * normalizer + offset
    }
}

struct ConvBN<Scalar: TensorFlowFloatingPoint>: Layer {
    var conv: Conv2D<Scalar>
    var norm: FABatchNorm<Scalar>
    init(
        filterShape: (Int, Int, Int, Int),
        strides: (Int, Int) = (1, 1),
        padding: Padding = .valid,
        activation: @escaping Conv2D<Scalar>.Activation = identity
    ) {
        // TODO (when control flow AD works): use Conv2D without bias
        self.conv = Conv2D(
            filterShape: filterShape,
            strides: strides,
            padding: padding,
            activation: activation)
        self.norm = FABatchNorm(featureCount: filterShape.3, epsilon: 1e-5)
    }

    @differentiable
    func applied(to input: Tensor<Scalar>, in context: Context) -> Tensor<Scalar> {
        return norm.applied(to: conv.applied(to: input, in: context), in: context)
    }
}

struct CnnModelBN: Layer {
    var reshapeToSquare = Reshape<Float>([-1, 28, 28, 1])
    var conv1 = ConvBN<Float>(
        filterShape: (5, 5, 1, 8),
        strides: (2, 2),
        padding: .same,
        activation: relu)
    var conv2 = ConvBN<Float>(
        filterShape: (3, 3, 8, 16),
        strides: (2, 2),
        padding: .same,
        activation: relu)
    var conv3 = ConvBN<Float>(
        filterShape: (3, 3, 16, 32),
        strides: (2, 2),
        padding: .same,
        activation: relu)
    var conv4 = ConvBN<Float>(
        filterShape: (3, 3, 32, 32),
        strides: (2, 2),
        padding: .same,
        activation: relu)
    
    var pool = AvgPool2D<Float>(poolSize: (2, 2), strides: (1, 1))
    
    var flatten = Flatten<Float>()
    var linear = Dense<Float>(inputSize: 32, outputSize: 10)
    
    @differentiable
    func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {
        // There isn't a "sequenced" defined with enough layers.
        let intermediate =  input.sequenced(
            in: context,
            through: reshapeToSquare, conv1, conv2, conv3, conv4)
        return intermediate.sequenced(in: context, through: pool, flatten, linear)
    }
}
