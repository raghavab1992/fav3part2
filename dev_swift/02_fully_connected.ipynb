{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/clattner/fastai_docs/dev_swift/FastaiNotebook_01a_fastai_layers\")\n",
      "\t\tFastaiNotebook_01a_fastai_layers\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpms8xwjbz\n",
      "/home/clattner/swift/usr/bin/swift-build: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/lib/swift/linux/libFoundation.so)\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 2.38s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_01a_fastai_layers' (3 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift-autolink-extract: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift-autolink-extract)\n",
      "\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 1>:1:18: error: consecutive statements on a line must be separated by ';'\n%install-location $cwd/swift-install\n                 ^\n                 ;\n\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_01a_fastai_layers\")' FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "\n",
    "// Typing Tensor<Float> all the time is tedious.  The S4TF team expects to \n",
    "// make \"Float\" be the default so we can just say \"Tensor\".  Until that happens\n",
    "// though, we can define our own alias.\n",
    "public typealias TF=Tensor<Float>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func normalize(_ x:TF, mean:TF, std:TF) -> TF {\n",
    "    return (x-mean)/std\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13066047 0.3081078\r\n"
     ]
    }
   ],
   "source": [
    "let trainMean = xTrain.mean()\n",
    "let trainStd  = xTrain.standardDeviation()\n",
    "print(trainMean, trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = normalize(xTrain, mean: trainMean, std: trainStd)\n",
    "xValid = normalize(xValid, mean: trainMean, std: trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func testNearZero(_ a: TF, tolerance: Float = 1e-3) {\n",
    "    assert(abs(a) < tolerance, \"Near zero: \\(a)\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(xTrain.mean())\n",
    "testNearZero(xTrain.standardDeviation() - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
    "let c = yTrain.max()+1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//num hidden\n",
    "let nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// simplified kaiming init / he init\n",
    "let w1 = TF(randomNormal: [m, nh]) / sqrt(Float(m))\n",
    "let b1 = TF(repeating: 0.0, shape: [nh])           // could also use zeros:\n",
    "let w2 = TF(randomNormal: [nh,1]) / sqrt(Float(nh))\n",
    "let b2 = TF(zeros: [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(w1.mean())\n",
    "testNearZero(w1.standardDeviation()-1/sqrt(Float(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "▿ 2 elements\n",
       "  - .0 : 0.0060178353\n",
       "  - .1 : 1.0077001\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This should be ~ (0,1) (mean,std)...\n",
    "(xValid.mean(),xValid.standardDeviation())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `@` in python we use `•` (or `matmul`) in Swift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lin(_ x: TF, _ w: TF, _ b: TF) -> TF { return x•w+b }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = lin(xValid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36849484 0.5603862\r\n"
     ]
    }
   ],
   "source": [
    "//...so should this, because we used kaiming init, which is designed to do this\n",
    "print(t.mean(), t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func myRelu(_ x:TF) -> TF { return max(x, 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36849484 0.5603862\r\n"
     ]
    }
   ],
   "source": [
    "//...actually it really should be this!\n",
    "print(t.mean(),t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// kaiming init / he init for relu\n",
    "let w1 = TF(randomNormal: [m,nh]) * sqrt(2.0/Float(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00019601914 0.05068941\r\n"
     ]
    }
   ],
   "source": [
    "print(w1.mean(), w1.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61064214 0.87908095\r\n"
     ]
    }
   ],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))\n",
    "print(t.mean(), t.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func model(_ xb: TF) -> TF {\n",
    "    let l1 = lin(xb, w1, b1)\n",
    "    let l2 = myRelu(l1)\n",
    "    let l3 = lin(l2, w2, b2)\n",
    "    return l3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.4166424999999999 ms,   min: 0.361097 ms,   max: 0.534531 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = model(xValid) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let preds = model(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func mse(_ out: TF, _ targ: TF) -> TF {\n",
    "    return (out.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert these to Float dtype.\n",
    "var yTrainF = TF(yTrain)\n",
    "var yValidF = TF(yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.742607\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients and backward pass\n",
    "\n",
    "Here we should how to calculate gradients for a simple model the hard way, manually.\n",
    "\n",
    "To store the gradients a bit like in PyTorch we introduce a `TensorWithGrad` class that has two attributes: the original tensor and the gradient. We choose a class to easily replicate the Python notebook: classes are reference types (which means they are mutable) while structures are value types.\n",
    "\n",
    "In fact, since this is the first time we're discovering Swift classes, let's jump into a [sidebar discussion about Value Semantics vs Reference Semantics](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5669969ead_0_145) since it is a pretty fundamental part of the programming model and a huge step forward that Swift takes.\n",
    "\n",
    "When we get back, we'll keep charging on, even though this is very non-idiomatic Swift code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// WARNING: This is designed to be similar to the PyTorch 02_fully_connected lesson,\n",
    "/// this isn't idiomatic Swift code.\n",
    "class TensorWithGrad {\n",
    "    var inner, grad:  TF\n",
    "    \n",
    "    init(_ x: TF) {\n",
    "        inner = x\n",
    "        grad = TF(zeros: x.shape)\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Redefine our functions on TensorWithGrad.\n",
    "func lin(_ x: TensorWithGrad, _ w: TensorWithGrad, _ b: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(matmul(x.inner, w.inner) + b.inner)\n",
    "}\n",
    "func myRelu(_ x: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(max(x.inner, 0))\n",
    "}\n",
    "func mse(_ inp: TensorWithGrad, _ targ: TF) -> TF {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    return (inp.inner.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define our gradient functions.\n",
    "func mseGrad(_ inp: TensorWithGrad, _ targ: TF) {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    inp.grad = 2.0 * (inp.inner.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.inner.shape[0])\n",
    "}\n",
    "\n",
    "func reluGrad(_ inp: TensorWithGrad, _ out: TensorWithGrad) {\n",
    "    //grad of relu with respect to input activations\n",
    "    inp.grad = (inp.inner .> 0).selecting(out.grad, TF(zeros: inp.inner.shape))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our python version (we've renamed the python `g` to `grad` for consistency):\n",
    "\n",
    "```python\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.grad = out.grad @ w.t()\n",
    "    w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0)\n",
    "    b.grad = out.grad.sum(0)\n",
    "```\n",
    "\n",
    "In Swift `@` is spelled `•`, which is <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or just use the `matmul()` function we've seen already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linGrad(_ inp:TensorWithGrad, _ out:TensorWithGrad, _ w:TensorWithGrad, _ b:TensorWithGrad){\n",
    "    // grad of linear layer with respect to input activations, weights and bias\n",
    "    inp.grad = out.grad • w.inner.transposed()\n",
    "    w.grad = inp.inner.transposed() • out.grad\n",
    "    b.grad = out.grad.sum(squeezingAxes: 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let w1a = TensorWithGrad(w1)\n",
    "let b1a = TensorWithGrad(b1)\n",
    "let w2a = TensorWithGrad(w2)\n",
    "let b2a = TensorWithGrad(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp:TensorWithGrad, _ targ:TF){\n",
    "    //forward pass:\n",
    "    let l1 = lin(inp, w1a, b1a)\n",
    "    let l2 = myRelu(l1)\n",
    "    let out = lin(l2, w2a, b2a)\n",
    "    //we don't actually need the loss in backward!\n",
    "    let loss = mse(out, targ)\n",
    "    \n",
    "    //backward pass:\n",
    "    mseGrad(out, targ)\n",
    "    linGrad(l2, out, w2a, b2a)\n",
    "    reluGrad(l1, l2)\n",
    "    linGrad(inp, l1, w1a, b1a)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let inp = TensorWithGrad(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardAndBackward(inp, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the S4TF Language Integrated Autodiff\n",
    "\n",
    "Let's compare to the language-integrated Swift for TensorFlow autodiff now. We have to mark the function as `@differentiable`.  This informs the compiler that we want it to automatically generate its gradients, and causes it to emit errors if there is anything contributing to the result of the function that cannot be differentiated.\n",
    "\n",
    "The `@differentiable` attribute is normally optional in a S4TF standalone environment, but is currently required in Jupyter notebooks.  The S4TF team is planning to relax this limitation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable\n",
    "func forward(_ inp: TF, _ targ: TF, w1: TF, b1: TF, w2: TF, b2: TF) -> TF {\n",
    "    let l1 = matmul(inp, w1) + b1\n",
    "    let l2 = relu(l1)\n",
    "    let l3 = matmul(l2, w2) + b2\n",
    "    return (l3.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can ask for the gradients w.r.t. any individual parameter like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let xGrad = gradient(at: xTrain) {xTrain in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let w1Grad = gradient(at: w1) {w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let b1Grad = gradient(at: b1) {b1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let w2Grad = gradient(at: w2) {w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let b2Grad = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree with the manually calculated gradients.\n",
    "testNearZero(xGrad - inp.grad)\n",
    "testNearZero(w1Grad - w1a.grad)\n",
    "testNearZero(b1Grad - b1a.grad)\n",
    "testNearZero(w2Grad - w2a.grad)\n",
    "testNearZero(b2Grad - b2a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also ask for gradients with respect to multiple things at the same time, but unfortunately, current AD bugs prevent getting more than two gradients at a time.  We can do a little bit better than the above code like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (xGrad2, w1Grad2) = gradient(at: xTrain, w1) {\n",
    "    xTrain, w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let (b1Grad2, w2Grad2) = gradient(at: b1, w2) {\n",
    "    b1, w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let b2Grad2 = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree.\n",
    "testNearZero(xGrad-xGrad2)\n",
    "testNearZero(w1Grad-w1Grad2)\n",
    "testNearZero(b1Grad-b1Grad2)\n",
    "testNearZero(w2Grad-w2Grad2)\n",
    "testNearZero(b2Grad-b2Grad2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently pretty ugly, and even when the bugs are fixed, it still won't be very idiomatic.  A more common thing is to wrap up all your parameters into a struct, and differentiate w.r.t. all of them at the same time (which, when we refactor the code, will be our model itself).\n",
    "\n",
    "Here is an example of that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct myParams: Differentiable {\n",
    "    public var x, w1, b1, w2, b2: TF\n",
    "}\n",
    "\n",
    "let allParams = myParams(x: xTrain, w1: w1, b1: b1, w2: w2, b2: b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can now get all of the gradients at once with a single call, and a single forward computation.\n",
    "let grads = gradient(at: allParams) {\n",
    "  allParams in\n",
    "    forward(allParams.x, yTrainF,\n",
    "            w1: allParams.w1, \n",
    "            b1: allParams.b1,\n",
    "            w2: allParams.w2, \n",
    "            b2: allParams.b2)\n",
    "}\n",
    "\n",
    "// Check that this still calculates the same thing.\n",
    "testNearZero(xGrad  - grads.x)\n",
    "testNearZero(w1Grad - grads.w1)\n",
    "testNearZero(b1Grad - grads.b1)\n",
    "testNearZero(w2Grad - grads.w2)\n",
    "testNearZero(b2Grad - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted the value for your loss as well as the gradients, you just have to use `valueWithGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (loss,grads) = valueWithGradient(at: allParams) { \n",
    "    allParams in forward(allParams.x, yTrainF, w1: allParams.w1, b1: allParams.b1, w2: allParams.w2, b2: allParams.b2)\n",
    "}\n",
    "\n",
    "testNearZero(xGrad  - grads.x)\n",
    "testNearZero(w1Grad - grads.w1)\n",
    "testNearZero(b1Grad - grads.b1)\n",
    "testNearZero(w2Grad - grads.w2)\n",
    "testNearZero(b2Grad - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of timing our implementation gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 8.455380100000001 ms,   min: 8.242054 ms,   max: 8.946601 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { forwardAndBackward(inp, yTrainF) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 10.136412199999999 ms,   min: 9.405311 ms,   max: 10.843894 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) {\n",
    "    _ = valueWithGradient(at: allParams) { \n",
    "        allParams in forward(allParams.x, \n",
    "                             yTrainF, \n",
    "                             w1: allParams.w1, \n",
    "                             b1: allParams.b1, \n",
    "                             w2: allParams.w2, \n",
    "                             b2: allParams.b2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: Path.cwd / \"02_fully_connected.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
