{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_03_minibatch_training\")' FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (Int(xTrain.shape[0]),Int(xTrain.shape[1]))\n",
    "let c = yTrain.max()+1\n",
    "print(n,m,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those can't be used to define a model cause they're not Ints though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct BasicModel: Layer {\n",
    "    public var layer1: Dense<Float>\n",
    "    public var layer2: Dense<Float>\n",
    "    \n",
    "    public init(nIn: Int, nHid: Int, nOut: Int){\n",
    "        layer1 = Dense(inputSize: nIn, outputSize: nHid, activation: relu)\n",
    "        layer2 = Dense(inputSize: nHid, outputSize: nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func applied(to input: Tensor<Float>, in context: Context) -> Tensor<Float> {\n",
    "        return input.sequenced(in: context, through: layer1, layer2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model = BasicModel(nIn: m, nHid: nHid, nOut: c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct DataBunch<Element> where Element: TensorGroup{\n",
    "    public var train: Dataset<Element>\n",
    "    public var valid: Dataset<Element>\n",
    "    \n",
    "    public init(train: Dataset<Element>, valid: Dataset<Element>) {\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func mnistDataBunch(path: Path = mnistPath, flat: Bool = false, bs: Int = 64\n",
    "                          ) -> DataBunch<DataBatch<Tensor<Float>, Tensor<Int32>>>{\n",
    "    let (xTrain,yTrain,xValid,yValid) = loadMNIST(path: path, flat: flat)\n",
    "    return DataBunch(train: Dataset(elements:DataBatch(xb:xTrain, yb:yTrain)).batched(Int64(bs)), \n",
    "                     valid: Dataset(elements:DataBatch(xb:xValid, yb:yValid)).batched(Int64(bs)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner (Marc's version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Please pick a better name for me! :-)\n",
    "enum CallbackException {\n",
    "    case cancelTraining\n",
    "    case cancelEpoch\n",
    "    case cancelBatch\n",
    "}\n",
    "\n",
    "enum CallbackEvent {\n",
    "    // I haven't implemented all the events.\n",
    "    case beginFit\n",
    "    case beginEpoch\n",
    "    case beginBatch\n",
    "    case beginValidate\n",
    "    case afterForwardsBackwards\n",
    "    case afterEpoch\n",
    "    case afterFit\n",
    "}\n",
    "\n",
    "func defaultCallback(e: CallbackEvent) {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner<Opt: Optimizer, Labels: TensorGroup>\n",
    "where Opt.Model.CotangentVector == Opt.Model.AllDifferentiableVariables,\n",
    "      Opt.Model.Input: TensorGroup\n",
    "{\n",
    "    typealias Model = Opt.Model\n",
    "    var model: Model\n",
    "    \n",
    "    typealias Inputs = Model.Input\n",
    "    // I'm getting some crashes in AD-generated code if I put a `lossFunc` in the learner.\n",
    "    // So I'm putting a `lossWithGradient` for now, to work around this.\n",
    "    // (model, context, inputs, labels) -> (loss, grad)\n",
    "    typealias LossWithGradient = (Model, Context, Inputs, Labels\n",
    "                                 ) -> (Tensor<Float>, Model.Output?, Model.AllDifferentiableVariables)\n",
    "    var lossWithGradient: LossWithGradient\n",
    "    \n",
    "    var optimizer: Opt\n",
    "    \n",
    "    typealias Data = DataBunch<DataBatch<Inputs, Labels>>\n",
    "    var data: Data\n",
    "    \n",
    "    var context: Context = Context(learningPhase: .training)\n",
    "\n",
    "    typealias Callback = (CallbackEvent) throws -> ()    \n",
    "    var callback: Callback = defaultCallback\n",
    "    \n",
    "    //Is there a better way tonitiliaze those to not make them Optionals?\n",
    "    var input: Model.Input? = nil\n",
    "    var target: Labels? = nil\n",
    "    var output: Model.Output? = nil\n",
    "    \n",
    "    var loss: Tensor<Float> = Tensor(0)\n",
    "    var grad: Model.AllDifferentiableVariables = Model.AllDifferentiableVariables.zero\n",
    "    \n",
    "    var inTrain: Bool = false\n",
    "    var epoch: Int = 0\n",
    "    var epochs: Int = 0\n",
    "    var nEpochs: Float = 0.0\n",
    "    var nIter: Int = 0\n",
    "    var iters: Int = 0\n",
    "    \n",
    "    init(\n",
    "        model: Model,\n",
    "        lossWithGradient: @escaping LossWithGradient,\n",
    "        optimizer: Opt,\n",
    "        data: Data\n",
    "    ) {\n",
    "        self.model = model\n",
    "        self.lossWithGradient = lossWithGradient\n",
    "        self.optimizer = optimizer\n",
    "        self.data = data\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's write the parts of the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension Learner{\n",
    "    func trainOneBatch(xb: Inputs, yb: Labels) throws {\n",
    "        try callback(.beginBatch)\n",
    "        (self.loss, self.output, self.grad) = lossWithGradient(model, self.context, xb, yb)\n",
    "        defer {\n",
    "            // Zero out the loss & gradient to ensure stale values aren't used.\n",
    "            self.loss = Tensor(0)\n",
    "            self.grad = Model.AllDifferentiableVariables.zero        \n",
    "        }\n",
    "        try callback(.afterForwardsBackwards)\n",
    "        if self.inTrain {optimizer.update(&model.allDifferentiableVariables, along: self.grad)}\n",
    "    }\n",
    "    \n",
    "    func trainOneEpoch() throws {\n",
    "        let ds = self.inTrain ? self.data.train : self.data.valid\n",
    "        self.iters = ds.count(where: {_ in true})\n",
    "        for batch in ds {\n",
    "            (self.input,self.target) = (batch.xb,batch.yb)\n",
    "            do { try trainOneBatch(xb: batch.xb, yb: batch.yb)} \n",
    "            catch CallbackException.cancelBatch {}  // Continue\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the whole fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension Learner{\n",
    "    func fit(epochs: Int) throws {\n",
    "        self.epochs = epochs\n",
    "        do {\n",
    "            try callback(.beginFit)\n",
    "            defer {\n",
    "                do { try callback(.afterFit) } \n",
    "                catch { print(\"Error during callback(.afterFit): \\(error)\")}\n",
    "            }\n",
    "            for epoch in 1...epochs {\n",
    "                self.epoch = epoch\n",
    "                try callback(.beginEpoch)\n",
    "                do { try trainOneEpoch()} \n",
    "                catch let error as CallbackException where error != .cancelTraining {}  // Continue\n",
    "                try callback(.beginValidate)\n",
    "                do { try trainOneEpoch()} \n",
    "                catch let error as CallbackException where error != .cancelTraining {}  // Continue\n",
    "                do { try callback(.afterEpoch) }\n",
    "                catch { print(\"Error during callback(.afterEpoch): \\(error)\")}\n",
    "            }\n",
    "        } catch is CallbackException {}  // Catch all CallbackExceptions.\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Helper so you don't need to do the chaining yourself. :-)\n",
    "func chainCallback<Opt, Labels>(on learner: Learner<Opt, Labels>, newCallback: @escaping (CallbackEvent) throws -> ()) {\n",
    "    let existingCallback = learner.callback\n",
    "    learner.callback = { event in\n",
    "        try newCallback(event)\n",
    "        try existingCallback(event)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func installTrainEval<Opt, Labels>(on learner: Learner<Opt, Labels>) {\n",
    "    chainCallback(on: learner) { event in\n",
    "        switch event {\n",
    "        case .beginFit:\n",
    "            learner.nEpochs = 0.0\n",
    "            learner.nIter = 0\n",
    "        case .beginEpoch:\n",
    "            print(\"Beginning epoch \\(learner.epoch)\")\n",
    "            learner.nEpochs = Float(learner.epoch)\n",
    "            learner.context = Context(learningPhase: .training)\n",
    "            learner.inTrain = true\n",
    "        case .afterForwardsBackwards:\n",
    "            if learner.inTrain{\n",
    "                learner.nEpochs += 1.0/Float(learner.iters)\n",
    "                learner.nIter   += 1\n",
    "            }\n",
    "        case .beginValidate:\n",
    "            learner.context = Context(learningPhase: .inference)\n",
    "            learner.inTrain = false\n",
    "        default: break\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class AverageMetrics {\n",
    "    public var metrics: [[Tensor<Float>]] = []\n",
    "    var count: Int = 0\n",
    "    var partials: [Tensor<Float>] = []\n",
    "}\n",
    "\n",
    "func installAverageMetric<Opt, Labels>(\n",
    "    _ metrics: [(Tensor<Float>, Tensor<Int32>) -> Tensor<Float>],\n",
    "    on learner: Learner<Opt, Labels>\n",
    "    ) -> AverageMetrics{\n",
    "    let avgMetrics = AverageMetrics()\n",
    "    chainCallback(on: learner) { event in\n",
    "        switch event {\n",
    "        case .beginEpoch:\n",
    "            avgMetrics.count = 0\n",
    "            avgMetrics.partials = Array(repeating: Tensor(0), count: metrics.count+1)\n",
    "        case .afterForwardsBackwards:\n",
    "            if !learner.inTrain{\n",
    "                if let target = learner.target as? Tensor<Int32>{\n",
    "                    avgMetrics.count += Int(target.shape[0])\n",
    "                    avgMetrics.partials[0] += Float(target.shape[0]) * learner.loss\n",
    "                    for i in 0..<metrics.count{\n",
    "                        avgMetrics.partials[i+1] += metrics[i]((learner.output as! Tensor<Float>), target) * Float(target.shape[0])\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        case .afterEpoch:\n",
    "            for i in 0..<metrics.count+1{\n",
    "                avgMetrics.partials[i] = avgMetrics.partials[i]/Float(avgMetrics.count)\n",
    "            }\n",
    "            avgMetrics.metrics.append(avgMetrics.partials)\n",
    "            print(avgMetrics.partials)\n",
    "        default: break\n",
    "        }\n",
    "    }\n",
    "    return avgMetrics\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lossWithGrad(\n",
    "    model: BasicModel,\n",
    "    in context: Context,\n",
    "    inputs: Tensor<Float>,\n",
    "    labels: Tensor<Int32>\n",
    ") -> (Tensor<Float>, BasicModel.Output, BasicModel.AllDifferentiableVariables) {\n",
    "    var outputs: BasicModel.Output? = nil\n",
    "    let (loss, grads) = model.valueWithGradient { model -> Tensor<Float> in\n",
    "        let predictions = model.applied(to: inputs, in: context)\n",
    "        outputs = predictions\n",
    "        return softmaxCrossEntropy(logits: predictions, labels: labels)\n",
    "    }\n",
    "    return (loss, outputs!, grads)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let opt = SGD<BasicModel, Float>(learningRate: 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(\n",
    "    model: model,\n",
    "    lossWithGradient: lossWithGrad,\n",
    "    optimizer: opt,\n",
    "    data: data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "installTrainEval(on: learner)\n",
    "let avgMetrics = installAverageMetric([accuracy], on: learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(epochs: 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
